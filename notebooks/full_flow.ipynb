{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fa8581",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "### [0. Global Vars](#0-global-vars)\n",
    "Configuration constants and system parameters for all three RAG stages\n",
    "\n",
    "### [1. Process PDFs](#1-process-pdfs)\n",
    "Document preprocessing pipeline with Vietnamese text optimization\n",
    "- [1.1 Text Simple Cleaner and Metadata](#11-text-simple-cleaner-and-metadata)\n",
    "- [1.2 Run PDF Processing Pipeline](#12-run-pdf-processing-pipeline)\n",
    "\n",
    "### [2. Build RAG](#2-build-rag)\n",
    "Vector database construction with hybrid BGE-M3 embeddings\n",
    "- [2.1 Milvus Client Connection](#21-milvus-client-connection)\n",
    "- [2.2 Milvus Builder and Search](#22-milvus-builder-and-search)\n",
    "- [2.3 Embedding Model and Builder Vector Store](#23-embedding-model-and-builder-vector-store)\n",
    "\n",
    "### [3. Retrieve from RAG](#3-retrieve-from-rag)\n",
    "Interactive retrieval system with LLM answer generation\n",
    "- [3.1 LLM Definition and Call Managements](#31-llm-definition-and-call-managements)\n",
    "- [3.2 Load retriever and models](#32-load-retriever-and-models)\n",
    "- [3.3 Define RAG Chain and Run](#33-define-rag-chain-and-run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd702b",
   "metadata": {},
   "source": [
    "___\n",
    "## 0. Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d993e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "CHARS_PER_TOKEN = 3\n",
    "CHUNKER_MODEL = \"BAAI/bge-m3\"\n",
    "MAX_CHUNK_TOKENS = 1024\n",
    "OVERLAP_TOKENS = 128\n",
    "\n",
    "# Build RAG\n",
    "DENSE_INDEX_CONFIG = {\n",
    "    \"index_type\": \"HNSW\",\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"params\": {\"M\": 16, \"efConstruction\": 64},\n",
    "}\n",
    "DENSE_INDEX_FALLBACK_CONFIG = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"params\": {\"nlist\": 256},\n",
    "}\n",
    "DENSE_SEARCH_FALLBACK_PARAMS = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 8}}\n",
    "DENSE_SEARCH_PARAMS = {\"metric_type\": \"IP\", \"params\": {\"drop_ratio_search\": 0.2}}\n",
    "MILVUS_DOCKER_URI = \"http://localhost:19530\"\n",
    "MILVUS_URI = \"data/milvus.db\"\n",
    "USE_DOCKER_MILVUS = False\n",
    "\n",
    "RRF_K = 30\n",
    "SPARSE_INDEX_CONFIG = {\n",
    "    \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "    \"metric_type\": \"IP\",\n",
    "    \"params\": {\"drop_ratio_build\": 0.2},\n",
    "}\n",
    "SPARSE_SEARCH_PARAMS = {\"metric_type\": \"IP\", \"params\": {\"drop_ratio_search\": 0.2}}\n",
    "\n",
    "COLLECTION_NAME = \"vndoc_rag_hybrid\"\n",
    "EMBED_MODEL_ID = CHUNKER_MODEL\n",
    "EMBEDDING_DIM = 1024\n",
    "ENCODE_KWARGS = {\n",
    "    \"normalize_embeddings\": True,\n",
    "    \"batch_size\": 8,\n",
    "    \"return_dense\": True,\n",
    "    \"return_sparse\": True,\n",
    "    \"return_colbert_vecs\": False,\n",
    "}\n",
    "\n",
    "# Retrieve\n",
    "MAX_OUTPUT_TOKENS = 4096\n",
    "TEMPERATURE = 0\n",
    "PROMPT = \"\"\"Bạn là một chuyên gia trí tuệ nhân tạo và học máy có kiến thức sâu rộng. Hãy trả lời câu hỏi dựa trên thông tin được cung cấp từ hệ thống RAG.\\n\\nCÂU HỎI: {query}\\n\\nTHÔNG TIN THAM KHẢO:\\n{context}\\n\\nHƯỚNG DẪN TRẢ LỜI:\\n1. Trả lời bằng tiếng Việt một cách chi tiết và rõ ràng\\n2. Chỉ sử dụng thông tin có trong các tài liệu được cung cấp - KHÔNG tự bịa đặt hoặc thêm thông tin\\n3. Mỗi nguồn tài liệu là riêng biệt - KHÔNG trộn lẫn hoặc kết hợp thông tin từ các nguồn khác nhau một cách tùy tiện\\n4. Nếu thông tin từ các nguồn khác nhau mâu thuẫn, hãy chỉ ra sự khác biệt này\\n5. Giải thích các thuật ngữ kỹ thuật bằng tiếng Việt\\n6. Trích dẫn rõ ràng nguồn thông tin cho mỗi phần trả lời\\n7. Nếu thông tin không đủ để trả lời đầy đủ, hãy thừa nhận điều này thay vì đoán\"\"\"\n",
    "\n",
    "DEFAULT_K = 10\n",
    "RERANK_TOP_K = 3\n",
    "SIMILARITY_THRESHOLD = 0.2\n",
    "RERANKER_MODEL_ID = \"BAAI/bge-reranker-v2-m3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab83e4",
   "metadata": {},
   "source": [
    "## 1. Process PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain.schema import Document\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "import hashlib\n",
    "import re\n",
    "import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b531619f",
   "metadata": {},
   "source": [
    "### 1.1 Text Simple Cleaner and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    \"\"\"Vietnamese text cleaner optimized for RAG preprocessing.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Allow Vietnamese letters + digits + punctuation\n",
    "        self.vietnamese_chars = (\n",
    "            \"àáạảãâầấậẩẫăằắặẳẵ\"\n",
    "            \"èéẹẻẽêềếệểễ\"\n",
    "            \"ìíịỉĩ\"\n",
    "            \"òóọỏõôồốộổỗơờớợởỡ\"\n",
    "            \"ùúụủũưừứựửữ\"\n",
    "            \"ỳýỵỷỹ\"\n",
    "            \"đ\"\n",
    "            \"ÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴ\"\n",
    "            \"ÈÉẸẺẼÊỀẾỆỂỄ\"\n",
    "            \"ÌÍỊỈĨ\"\n",
    "            \"ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ\"\n",
    "            \"ÙÚỤỦŨƯỪỨỰỬỮ\"\n",
    "            \"ỲÝỴỶỸ\"\n",
    "            \"Đ\"\n",
    "        )\n",
    "\n",
    "    def normalize_unicode(self, text: str) -> str:\n",
    "        \"\"\"Normalize Unicode (NFC) for consistency.\"\"\"\n",
    "        return unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    def clean_text(self, text: str) -> tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"Clean text and return metadata for RAG preparation.\"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return \"\", {\"was_empty\": True}\n",
    "\n",
    "        gc.collect()\n",
    "        original_length = len(text)\n",
    "\n",
    "        # Unicode normalization\n",
    "        text = self.normalize_unicode(text)\n",
    "\n",
    "        # Basic noise cleanup\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)  # collapse spaces/tabs\n",
    "        text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)  # normalize paragraph breaks\n",
    "        text = re.sub(r\"[.]{3,}\", \"...\", text)  # reduce ellipses\n",
    "        text = re.sub(r\"[-]{3,}\", \"—\", text)  # convert long dashes\n",
    "        text = re.sub(r\"[,]{2,}\", \",\", text)  # collapse commas\n",
    "\n",
    "        # Remove stray non-text characters (OCR artifacts, control chars)\n",
    "        vietnamese_escaped = re.escape(self.vietnamese_chars)\n",
    "        allowed_pattern = f\"[a-zA-Z0-9{vietnamese_escaped}\\\\s.,;:!?()\\\\[\\\\]{{}}\\\"'\\\\-_/\\\\\\\\+=%&@#$\\\\n\\\\t]\"\n",
    "        text = \"\".join(ch for ch in text if re.match(allowed_pattern, ch))\n",
    "\n",
    "        # Normalize line breaks around punctuation (fix OCR issues)\n",
    "        text = re.sub(r\"\\s*([.,;:!?])\\s*\", r\"\\1 \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        cleaned_text = text.strip()\n",
    "\n",
    "        # Metadata for tracking\n",
    "        cleaning_metadata = {\n",
    "            \"was_empty\": False,\n",
    "            \"original_length\": original_length,\n",
    "            \"cleaned_length\": len(cleaned_text),\n",
    "            \"word_count\": len(cleaned_text.split()),\n",
    "            \"paragraphs\": cleaned_text.count(\"\\n\\n\") + 1,\n",
    "            \"estimated_tokens\": self.estimate_tokens(cleaned_text),\n",
    "        }\n",
    "\n",
    "        return cleaned_text, cleaning_metadata\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Rough token estimation (tune per tokenizer).\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "        return len(text) // CHARS_PER_TOKEN + 1\n",
    "\n",
    "\n",
    "class ChunkMetadataGenerator:\n",
    "    \"\"\"Generates core metadata for chunks\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session_id = str(uuid.uuid4())[:8]\n",
    "        self.chunk_counter = 0\n",
    "\n",
    "    def generate_chunk_id(self, source_file: str, chunk_index: int) -> str:\n",
    "        \"\"\"Generate unique chunk ID\"\"\"\n",
    "        file_hash = hashlib.md5(source_file.encode()).hexdigest()[:8]\n",
    "        return f\"CHUNK_{self.session_id}_{file_hash}_{chunk_index:04d}\"\n",
    "\n",
    "    def generate_document_id(self, source_file: str) -> str:\n",
    "        \"\"\"Generate unique document ID\"\"\"\n",
    "        file_hash = hashlib.md5(source_file.encode()).hexdigest()[:8]\n",
    "        return f\"DOC_{self.session_id}_{file_hash}\"\n",
    "\n",
    "    def create_metadata(\n",
    "        self,\n",
    "        doc: Document,\n",
    "        source_file: str,\n",
    "        chunk_index: int,\n",
    "        cleaning_metadata: Dict[str, Any],\n",
    "        estimated_tokens: int,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create chunk metadata\"\"\"\n",
    "        gc.collect()\n",
    "        self.chunk_counter += 1\n",
    "\n",
    "        # Generate IDs\n",
    "        chunk_id = self.generate_chunk_id(source_file, chunk_index)\n",
    "        document_id = self.generate_document_id(source_file)\n",
    "\n",
    "        # Extract file information\n",
    "        file_path = Path(source_file)\n",
    "        content = doc.page_content\n",
    "        content_hash = hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "        # Core metadata\n",
    "        metadata = {\n",
    "            # Essential IDs\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"document_id\": document_id,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"global_index\": self.chunk_counter,\n",
    "            # Processing info\n",
    "            \"session_id\": self.session_id,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"chunker_model\": CHUNKER_MODEL,\n",
    "            # Source info\n",
    "            \"source_file\": source_file,\n",
    "            \"source_filename\": str(file_path.name),\n",
    "            # Content metrics\n",
    "            \"content_length\": len(content),\n",
    "            \"estimated_tokens\": estimated_tokens,\n",
    "            \"word_count\": cleaning_metadata.get(\"word_count\", 0),\n",
    "            \"content_hash\": content_hash,\n",
    "            # # Future use placeholders\n",
    "            # \"summary\": None,\n",
    "            # \"keywords\": None,\n",
    "            # \"embedding_id\": None,\n",
    "            # \"last_accessed\": None,\n",
    "        }\n",
    "\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab95c88",
   "metadata": {},
   "source": [
    "### 1.2 Run PDF Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc57bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleaner()\n",
    "metadata_generator = ChunkMetadataGenerator()\n",
    "\n",
    "pdf_files: List[str] = []\n",
    "all_documents: List[Document] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf914d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = HybridChunker(\n",
    "    tokenizer=CHUNKER_MODEL,\n",
    "    max_tokens=MAX_CHUNK_TOKENS,\n",
    "    overlap_tokens=OVERLAP_TOKENS,\n",
    ")\n",
    "\n",
    "# Process each PDF file\n",
    "pbar = tqdm(total=len(pdf_files), desc=\"Processing PDFs\")\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pbar.set_description(f\"Processing {pdf_file.name}\")\n",
    "    pdf_file_str = str(pdf_file)\n",
    "\n",
    "    try:\n",
    "        # Use DoclingLoader with chunker\n",
    "        loader = DoclingLoader(\n",
    "            file_path=pdf_file_str,\n",
    "            export_type=ExportType.DOC_CHUNKS,\n",
    "            chunker=chunker,\n",
    "        )\n",
    "\n",
    "        # Load documents - already chunked\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Process each chunk\n",
    "        file_chunks_processed = 0\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            # Clean text\n",
    "            cleaned_content, cleaning_metadata = text_cleaner.clean_text(\n",
    "                doc.page_content\n",
    "            )\n",
    "\n",
    "            if cleaned_content and len(cleaned_content) > 50:\n",
    "                estimated_tokens = text_cleaner.estimate_tokens(cleaned_content)\n",
    "\n",
    "                # Create metadata\n",
    "                metadata = metadata_generator.create_metadata(\n",
    "                    doc=doc,\n",
    "                    source_file=pdf_file_str,\n",
    "                    chunk_index=i,\n",
    "                    cleaning_metadata=cleaning_metadata,\n",
    "                    estimated_tokens=estimated_tokens,\n",
    "                )\n",
    "\n",
    "                # Update document with cleaned content and metadata\n",
    "                doc.page_content = cleaned_content\n",
    "                doc.metadata = metadata\n",
    "\n",
    "                all_documents.append(doc)\n",
    "                file_chunks_processed += 1\n",
    "\n",
    "        del loader\n",
    "        pbar.update(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "del chunker\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81680ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return out: `all_documents`\n",
    "display(all_documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dec205",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Build RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import Union\n",
    "from pymilvus import (\n",
    "    AnnSearchRequest,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    FieldSchema,\n",
    "    MilvusClient,\n",
    "    RRFRanker,\n",
    "    connections,\n",
    ")\n",
    "from pymilvus.milvus_client.index import IndexParams\n",
    "from FlagEmbedding import BGEM3FlagModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0570b8",
   "metadata": {},
   "source": [
    "### 2.1 Milvus Client Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e415832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusConnectionManager:\n",
    "    \"\"\"Manages Milvus connections with Docker support\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client: Optional[MilvusClient] = None\n",
    "        self.supports_hnsw = False\n",
    "\n",
    "    def get_client(self) -> Tuple[MilvusClient, bool]:\n",
    "        \"\"\"Get Milvus client with automatic connection management\"\"\"\n",
    "        if self.client is not None:\n",
    "            return self.client, self.supports_hnsw\n",
    "\n",
    "        # Try Docker connection first if enabled\n",
    "        if USE_DOCKER_MILVUS:\n",
    "            try:\n",
    "                print(\"🐳 Connecting to Docker Milvus...\")\n",
    "                self.client = MilvusClient(uri=MILVUS_DOCKER_URI)\n",
    "                # Test connection\n",
    "                self.client.list_collections()\n",
    "                self.supports_hnsw = True\n",
    "                print(\"✅ Docker Milvus connected (HNSW enabled)\")\n",
    "                return self.client, True\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Docker connection failed: {e}\")\n",
    "\n",
    "        # Fallback to local file-based connection\n",
    "        print(\"📁 Using local file-based Milvus...\")\n",
    "        self.client = MilvusClient(uri=str(MILVUS_URI))\n",
    "        self.supports_hnsw = False\n",
    "        print(\"✅ Local Milvus connected\")\n",
    "        return self.client, False\n",
    "\n",
    "    def get_index_config(self):\n",
    "        \"\"\"Get appropriate index configuration\"\"\"\n",
    "        if self.supports_hnsw:\n",
    "            return DENSE_INDEX_CONFIG, DENSE_SEARCH_PARAMS\n",
    "        else:\n",
    "            return DENSE_INDEX_FALLBACK_CONFIG, DENSE_SEARCH_FALLBACK_PARAMS\n",
    "\n",
    "\n",
    "# Global connection manager\n",
    "_connection_manager = MilvusConnectionManager()\n",
    "\n",
    "\n",
    "def get_milvus_client() -> Tuple[MilvusClient, bool]:\n",
    "    \"\"\"Get the global Milvus client instance\"\"\"\n",
    "    return _connection_manager.get_client()\n",
    "\n",
    "\n",
    "def get_index_config():\n",
    "    \"\"\"Get appropriate index configuration for current connection\"\"\"\n",
    "    return _connection_manager.get_index_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eea7cd",
   "metadata": {},
   "source": [
    "### 2.2 Milvus Builder and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d82fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_hybrid_collection(client: MilvusClient, name: str, dense_dim: int) -> bool:\n",
    "    \"\"\"\n",
    "    Ensure hybrid collection exists with proper schema.\n",
    "\n",
    "    Returns:\n",
    "        True if collection was created/recreated, False if it already existed\n",
    "    \"\"\"\n",
    "    if client.has_collection(name):\n",
    "        client.drop_collection(name)\n",
    "\n",
    "    schema = CollectionSchema(\n",
    "        fields=[\n",
    "            FieldSchema(\n",
    "                name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=64\n",
    "            ),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "            FieldSchema(\n",
    "                name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=dense_dim\n",
    "            ),\n",
    "            FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "            FieldSchema(name=\"metadata\", dtype=DataType.JSON),\n",
    "        ],\n",
    "        description=\"Hybrid dense+sparse collection for BGE-M3\",\n",
    "    )\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=name, schema=schema, consistency_level=\"Strong\"\n",
    "    )\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def build_indexes(client: MilvusClient, name: str) -> None:\n",
    "    \"\"\"Build indexes with automatic HNSW/IVF_FLAT selection\"\"\"\n",
    "    try:\n",
    "        # Get appropriate index configuration\n",
    "        dense_index_config, dense_search_params = get_index_config()\n",
    "\n",
    "        # Dense index with auto-detection\n",
    "        dense_index_params = IndexParams()\n",
    "        dense_index_params.add_index(\n",
    "            field_name=\"dense_vector\",\n",
    "            index_type=dense_index_config[\"index_type\"],\n",
    "            metric_type=dense_index_config[\"metric_type\"],\n",
    "            params=dense_index_config[\"params\"],\n",
    "        )\n",
    "        client.create_index(\n",
    "            collection_name=name,\n",
    "            index_params=dense_index_params,\n",
    "        )\n",
    "\n",
    "        # Sparse index using config\n",
    "        sparse_index_params = IndexParams()\n",
    "        sparse_index_params.add_index(\n",
    "            field_name=\"sparse_vector\",\n",
    "            index_type=SPARSE_INDEX_CONFIG[\"index_type\"],\n",
    "            metric_type=SPARSE_INDEX_CONFIG[\"metric_type\"],\n",
    "            params=SPARSE_INDEX_CONFIG[\"params\"],\n",
    "        )\n",
    "        client.create_index(\n",
    "            collection_name=name,\n",
    "            index_params=sparse_index_params,\n",
    "        )\n",
    "        client.load_collection(name)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "\n",
    "def insert_documents(\n",
    "    client: MilvusClient,\n",
    "    name: str,\n",
    "    dense_vecs,\n",
    "    sparse_vecs,\n",
    "    docs: List[Document],\n",
    ") -> None:\n",
    "    \"\"\"Insert documents with better error handling\"\"\"\n",
    "    try:\n",
    "        rows = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"id\": uuid.uuid4().hex,\n",
    "                    \"text\": doc.page_content,\n",
    "                    \"dense_vector\": dense_vecs[i].tolist(),\n",
    "                    \"sparse_vector\": sparse_vecs[i],  # dict {token_id: weight}\n",
    "                    \"metadata\": doc.metadata or {},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        result = client.insert(collection_name=name, data=rows)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    uri: str,\n",
    "    name: str,\n",
    "    dense_q,\n",
    "    sparse_q,\n",
    "    k: int = 20,\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Perform hybrid search with automatic parameter selection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure connection for Collection API\n",
    "        connection_alias = \"default\"\n",
    "        try:\n",
    "            connections.get_connection(alias=connection_alias)\n",
    "        except Exception:\n",
    "            connections.connect(alias=connection_alias, uri=uri)\n",
    "\n",
    "        coll = Collection(name)\n",
    "\n",
    "        # Get appropriate search parameters\n",
    "        _, dense_search_params = get_index_config()\n",
    "\n",
    "        # Create search requests using appropriate parameters\n",
    "        dense_req = AnnSearchRequest(\n",
    "            data=[dense_q],\n",
    "            anns_field=\"dense_vector\",\n",
    "            param=dense_search_params,\n",
    "            limit=k,\n",
    "        )\n",
    "\n",
    "        sparse_req = AnnSearchRequest(\n",
    "            data=[sparse_q],\n",
    "            anns_field=\"sparse_vector\",\n",
    "            param=SPARSE_SEARCH_PARAMS,\n",
    "            limit=k,\n",
    "        )\n",
    "\n",
    "        # Create RRF ranker using config\n",
    "        ranker = RRFRanker(k=RRF_K)\n",
    "\n",
    "        # Perform hybrid search\n",
    "        res = coll.hybrid_search(\n",
    "            reqs=[dense_req, sparse_req],\n",
    "            rerank=ranker,\n",
    "            limit=k,\n",
    "            output_fields=[\"text\", \"metadata\"],\n",
    "        )\n",
    "\n",
    "        # Convert to (Document, score)\n",
    "        out: List[Tuple[Document, float]] = []\n",
    "        for hit in res[0]:\n",
    "            # PyMilvus hit object structure\n",
    "            meta = hit.entity.get(\"metadata\", {})\n",
    "            text = hit.entity.get(\"text\", \"\")\n",
    "            score = float(hit.distance)\n",
    "            out.append((Document(page_content=text, metadata=meta), score))\n",
    "\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in hybrid search: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_dense(\n",
    "    client: MilvusClient,\n",
    "    collection_name: str,\n",
    "    query_embedding: List[float],\n",
    "    k: int = 20,\n",
    "    search_params: Optional[Dict] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform dense vector search.\n",
    "\n",
    "    Args:\n",
    "        client: Milvus client\n",
    "        collection_name: Name of the collection\n",
    "        query_embedding: Dense query embedding\n",
    "        k: Number of results to return\n",
    "        search_params: Search parameters\n",
    "\n",
    "    Returns:\n",
    "        List of search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_params = search_params or DENSE_SEARCH_PARAMS\n",
    "\n",
    "        results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"dense_vector\",\n",
    "            search_params=search_params,\n",
    "            limit=k,\n",
    "            output_fields=[\"text\", \"metadata\"],\n",
    "        )\n",
    "\n",
    "        # Convert to standard format\n",
    "        formatted_results = []\n",
    "        for i, hit in enumerate(results[0]):\n",
    "            formatted_results.append(\n",
    "                {\n",
    "                    \"id\": hit.id,  # Use attribute, not dict access\n",
    "                    \"dense_score\": float(hit.distance),  # Use attribute directly\n",
    "                    \"rank\": i + 1,\n",
    "                    \"entity\": hit.entity,  # hit.entity is already a dict\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_sparse(\n",
    "    client: MilvusClient,\n",
    "    collection_name: str,\n",
    "    query_sparse: Dict,\n",
    "    k: int = 20,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform sparse vector search.\n",
    "\n",
    "    Args:\n",
    "        client: Milvus client\n",
    "        collection_name: Name of the collection\n",
    "        query_sparse: Sparse query embedding (dict of token_id -> weight)\n",
    "        k: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        List of search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            data=[query_sparse],\n",
    "            anns_field=\"sparse_vector\",\n",
    "            search_params=SPARSE_SEARCH_PARAMS,\n",
    "            limit=k,\n",
    "            output_fields=[\"text\", \"metadata\"],\n",
    "        )\n",
    "\n",
    "        # Convert to standard format\n",
    "        formatted_results = []\n",
    "        for i, hit in enumerate(results[0]):\n",
    "            formatted_results.append(\n",
    "                {\n",
    "                    \"id\": hit.id,  # Use attribute, not dict access\n",
    "                    \"sparse_score\": float(hit.distance),  # Use attribute directly\n",
    "                    \"rank\": i + 1,\n",
    "                    \"entity\": hit.entity,  # hit.entity is already a dict\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    dense_results: List[Dict],\n",
    "    sparse_results: List[Dict],\n",
    "    k: int = 10,\n",
    "    dense_weight: float = 0.7,\n",
    "    sparse_weight: float = 0.3,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Simplified RRF fusion with better scoring.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc_scores = {}\n",
    "        all_docs = {}\n",
    "\n",
    "        # Process dense results\n",
    "        for rank, result in enumerate(dense_results, 1):\n",
    "            doc_id = result.get(\"id\")\n",
    "            if doc_id:\n",
    "                rrf_score = dense_weight / (k + rank)\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score\n",
    "                all_docs[doc_id] = result\n",
    "                all_docs[doc_id][\"dense_score\"] = result.get(\"dense_score\", 0)\n",
    "\n",
    "        # Process sparse results\n",
    "        for rank, result in enumerate(sparse_results, 1):\n",
    "            doc_id = result.get(\"id\")\n",
    "            if doc_id:\n",
    "                rrf_score = sparse_weight / (k + rank)\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score\n",
    "\n",
    "                if doc_id in all_docs:\n",
    "                    all_docs[doc_id][\"sparse_score\"] = result.get(\"sparse_score\", 0)\n",
    "                else:\n",
    "                    all_docs[doc_id] = result\n",
    "                    all_docs[doc_id][\"sparse_score\"] = result.get(\"sparse_score\", 0)\n",
    "                    all_docs[doc_id][\"dense_score\"] = 0\n",
    "\n",
    "        # Normalize scores to 0-1 range\n",
    "        if doc_scores:\n",
    "            max_score = max(doc_scores.values())\n",
    "            min_score = min(doc_scores.values())\n",
    "            score_range = max_score - min_score\n",
    "\n",
    "            if score_range > 0:\n",
    "                for doc_id in doc_scores:\n",
    "                    normalized = (doc_scores[doc_id] - min_score) / score_range\n",
    "                    doc_scores[doc_id] = 0.1 + 0.9 * normalized\n",
    "            else:\n",
    "                for doc_id in doc_scores:\n",
    "                    doc_scores[doc_id] = 0.5\n",
    "\n",
    "        # Sort and format results\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        fused_results = []\n",
    "        for doc_id, rrf_score in sorted_docs:\n",
    "            doc = all_docs[doc_id].copy()\n",
    "            doc[\"rrf_score\"] = rrf_score\n",
    "            doc[\"combined_score\"] = rrf_score\n",
    "            fused_results.append(doc)\n",
    "\n",
    "        return fused_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in RRF fusion: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9470f",
   "metadata": {},
   "source": [
    "### 2.3 Embedding Model and Builder Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGEM3Encoder:\n",
    "    \"\"\"Encode text with BGE-M3 producing dense and sparse vectors.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"BAAI/bge-m3\",\n",
    "        device: str = \"cpu\",\n",
    "        normalize_embeddings: bool = True,\n",
    "        use_fp16: bool = False,  # Set to False to avoid dtype mismatches\n",
    "        max_length: int = 512,\n",
    "        batch_size: int = 32,\n",
    "        trust_remote_code: bool = True,\n",
    "    ) -> None:\n",
    "        self.model_id = model\n",
    "        self.device = device\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.model = BGEM3FlagModel(\n",
    "            model,\n",
    "            device=device,\n",
    "            use_fp16=use_fp16,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "        )\n",
    "\n",
    "    def encode(\n",
    "        self, text_or_texts: Union[str, List[str]], batch_size: Optional[int] = None\n",
    "    ) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        Encode text(s) to dense and sparse vectors.\n",
    "\n",
    "        Args:\n",
    "            text_or_texts: Single text string or list of texts\n",
    "            batch_size: Batch size for encoding\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with 'dense_vecs' and 'lexical_weights' keys\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(text_or_texts, str):\n",
    "                texts = [text_or_texts]\n",
    "            else:\n",
    "                texts = text_or_texts\n",
    "\n",
    "            if not texts:\n",
    "                raise ValueError(\"No texts provided for encoding\")\n",
    "\n",
    "            out = self.model.encode(\n",
    "                sentences=texts,\n",
    "                batch_size=batch_size or self.batch_size,\n",
    "                max_length=self.max_length,\n",
    "                return_dense=True,\n",
    "                return_sparse=True,\n",
    "                return_colbert_vecs=False,\n",
    "            )\n",
    "\n",
    "            # Ensure consistent data types (convert to float32 if needed)\n",
    "            if \"dense_vecs\" in out:\n",
    "                import numpy as np\n",
    "\n",
    "                out[\"dense_vecs\"] = out[\"dense_vecs\"].astype(np.float32)\n",
    "\n",
    "            # Validate output structure\n",
    "            if \"dense_vecs\" not in out or \"lexical_weights\" not in out:\n",
    "                raise ValueError(\"Invalid output from BGE-M3 model\")\n",
    "\n",
    "            # out keys: \"dense_vecs\": np.ndarray [n, d], \"lexical_weights\": List[Dict[token_id->weight]]\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error encoding with BGE-M3: {e}\")\n",
    "            # Return empty structure on error\n",
    "            import numpy as np\n",
    "\n",
    "            return {\n",
    "                \"dense_vecs\": np.array([]).astype(np.float32),\n",
    "                \"lexical_weights\": [],\n",
    "            }\n",
    "\n",
    "    def encode_query(self, text: str) -> Dict[str, List]:\n",
    "        \"\"\"Encode a single query text.\"\"\"\n",
    "        return self.encode(text)\n",
    "\n",
    "\n",
    "def load_encoder() -> BGEM3Encoder:\n",
    "    \"\"\"Load BGE-M3 encoder\"\"\"\n",
    "    return BGEM3Encoder(\n",
    "        model=EMBED_MODEL_ID,\n",
    "        device=\"cpu\",\n",
    "        normalize_embeddings=ENCODE_KWARGS.get(\"normalize_embeddings\", True),\n",
    "        batch_size=ENCODE_KWARGS.get(\"batch_size\", 32),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_hybrid_vectorstore(documents: List[Document], batch_size: int = 32) -> bool:\n",
    "    \"\"\"Build hybrid vectorstore with automatic Docker/local detection\"\"\"\n",
    "    if not documents:\n",
    "        print(\"❌ No documents to process\")\n",
    "        return False\n",
    "\n",
    "    print(f\"🔄 Building hybrid vectorstore with {len(documents)} documents...\")\n",
    "\n",
    "    # Get client with automatic connection management\n",
    "    client, supports_hnsw = get_milvus_client()\n",
    "\n",
    "    # Create collection\n",
    "    ensure_hybrid_collection(client, COLLECTION_NAME, EMBEDDING_DIM)\n",
    "\n",
    "    # Load encoder\n",
    "    encoder = load_encoder()\n",
    "\n",
    "    # Process documents in batches\n",
    "    total = len(documents)\n",
    "    total_batches = (total + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx, start in enumerate(range(0, total, batch_size)):\n",
    "        end = min(start + batch_size, total)\n",
    "        batch_docs = documents[start:end]\n",
    "        texts = [d.page_content for d in batch_docs]\n",
    "\n",
    "        print(\n",
    "            f\"📦 Processing batch {batch_idx + 1}/{total_batches} ({len(batch_docs)} docs)\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Encode texts\n",
    "            emb = encoder.encode(texts, batch_size=len(texts))\n",
    "            dense = emb[\"dense_vecs\"]\n",
    "            sparse = emb[\"lexical_weights\"]\n",
    "\n",
    "            # Insert into Milvus\n",
    "            insert_documents(client, COLLECTION_NAME, dense, sparse, batch_docs)\n",
    "\n",
    "            # Clean up memory\n",
    "            del emb, dense, sparse, batch_docs, texts\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing batch {batch_idx + 1}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Build indexes\n",
    "    print(\"🔧 Building indexes...\")\n",
    "    build_indexes(client, COLLECTION_NAME)\n",
    "\n",
    "    index_type = \"HNSW\" if supports_hnsw else \"IVF_FLAT\"\n",
    "    print(f\"✅ Hybrid vectorstore built successfully with {index_type} indexing!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acee69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = build_hybrid_vectorstore(all_documents, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b2ed2",
   "metadata": {},
   "source": [
    "___\n",
    "## 3. Retrieve from RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ee49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import Client, types\n",
    "from FlagEmbedding import FlagReranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd565754",
   "metadata": {},
   "source": [
    "### 3.1 LLM Definition and Call Managements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74526ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLM(ABC):\n",
    "    \"\"\"Base class for all LLM implementations\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        api_key: Optional[str] = None,\n",
    "        max_tokens: int = MAX_OUTPUT_TOKENS,\n",
    "        temperature: float = TEMPERATURE,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize LLM with common parameters.\n",
    "\n",
    "        Args:\n",
    "            model_id: Model identifier\n",
    "            api_key: API key for the service\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.api_key = api_key\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate response from prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "\n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class GeminiLLM:\n",
    "    \"\"\"Google Gemini LLM implementation\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"gemini-2.5-flash\",\n",
    "        api_key: Optional[str] = None,\n",
    "        max_tokens: int = MAX_OUTPUT_TOKENS,\n",
    "        temperature: float = TEMPERATURE,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Gemini LLM.\n",
    "\n",
    "        Args:\n",
    "            model_id: Gemini model identifier\n",
    "            api_key: API key (gets from env if None)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "        \"\"\"\n",
    "        load_dotenv()\n",
    "        self.model_id = model_id\n",
    "        self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY is required\")\n",
    "\n",
    "        self.llm = Client(api_key=self.api_key)\n",
    "        self.config = types.GenerateContentConfig(\n",
    "            system_instruction=\"You are a helpful assistant.\",\n",
    "            max_output_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            # thinking_config=types.ThinkingConfig(\n",
    "            #     max_steps=5,\n",
    "            #     stop_sequences=[\"\\n\"],\n",
    "            # ),\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate response from prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "\n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.llm.models.generate_content(\n",
    "                model=self.model_id,\n",
    "                contents=prompt,\n",
    "                config=self.config,\n",
    "            )\n",
    "            result = response.text.strip()\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "\n",
    "\n",
    "class LLMFactory:\n",
    "    \"\"\"Factory for creating LLM model instances\"\"\"\n",
    "\n",
    "    # Register available models here\n",
    "    MODELS = {\n",
    "        \"gemini\": GeminiLLM,\n",
    "        # \"watsonx\": WatsonxLLM,\n",
    "        # \"your_model\": YourModelLLM,\n",
    "        # Add more models here\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def create_llm(\n",
    "        self, model_type: str = \"gemini\", model_id: Optional[str] = None, **kwargs\n",
    "    ) -> BaseLLM:\n",
    "        \"\"\"\n",
    "        Create an LLM instance.\n",
    "\n",
    "        Args:\n",
    "            model_type: Type of model (\"gemini\", \"your_model\", etc.)\n",
    "            model_id: Specific model ID (uses default if None)\n",
    "            **kwargs: Additional parameters for the model\n",
    "\n",
    "        Returns:\n",
    "            Initialized LLM instance\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If model_type is not supported\n",
    "        \"\"\"\n",
    "        if model_type not in self.MODELS:\n",
    "            available = \", \".join(self.MODELS.keys())\n",
    "            raise ValueError(\n",
    "                f\"Model '{model_type}' not supported. Available: {available}\"\n",
    "            )\n",
    "\n",
    "        model_class = self.MODELS[model_type]\n",
    "\n",
    "        # Set default model_id based on type\n",
    "        if model_id is None:\n",
    "            defaults = {\n",
    "                \"gemini\": \"gemini-2.5-flash\",\n",
    "                # \"watsonx\": \"ibm/granite-13b-chat-v2\",\n",
    "                # Add defaults for other models\n",
    "            }\n",
    "            model_id = defaults.get(model_type, \"default-model\")\n",
    "\n",
    "        return model_class(model_id=model_id, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def list_available_models(cls) -> list:\n",
    "        \"\"\"Get list of available model types\"\"\"\n",
    "        return list(cls.MODELS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGLLMCaller:\n",
    "    \"\"\"\n",
    "    All-in-one LLM caller: prepare prompt, init model, call LLM, return structured output.\n",
    "\n",
    "    Now supports multiple model types through the factory system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_type: str = \"gemini\", **model_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize RAG LLM caller.\n",
    "\n",
    "        Args:\n",
    "            model_type: Type of model to use (\"gemini\", etc.)\n",
    "            **model_kwargs: Additional parameters for the model\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model_kwargs = model_kwargs\n",
    "        self.llm: Optional[BaseLLM] = None\n",
    "\n",
    "    def _init_llm(self, **kwargs):\n",
    "        \"\"\"Initialize LLM if not already done\"\"\"\n",
    "        if self.llm is None:\n",
    "            # Combine init kwargs with runtime kwargs\n",
    "            combined_kwargs = {**self.model_kwargs, **kwargs}\n",
    "            self.llm = LLMFactory.create_llm(self.model_type, **combined_kwargs)\n",
    "\n",
    "    def _prepare_context(self, documents: List[Document]) -> str:\n",
    "        \"\"\"Format documents into context string\"\"\"\n",
    "        if not documents:\n",
    "            return \"Không tìm thấy thông tin liên quan.\"\n",
    "\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            content = doc.page_content.strip()\n",
    "            if content:\n",
    "                context_parts.append(f\"[Tài liệu {i}]:\\n{content}\")\n",
    "\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    def _prepare_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create final prompt from query and context\"\"\"\n",
    "        return PROMPT.format(context=context, query=query)\n",
    "\n",
    "    def generate_answer(\n",
    "        self, query: str, documents: List[Document], **llm_kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete pipeline: documents → context → prompt → LLM → structured output\n",
    "\n",
    "        Args:\n",
    "            query: User question\n",
    "            documents: Retrieved documents\n",
    "            **llm_kwargs: Optional LLM parameters (model_id, max_tokens, temperature)\n",
    "\n",
    "        Returns:\n",
    "            Dict with answer, context_length, success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Prepare context\n",
    "            context = self._prepare_context(documents)\n",
    "\n",
    "            # Step 2: Prepare prompt\n",
    "            prompt = self._prepare_prompt(query, context)\n",
    "\n",
    "            # Step 3: Initialize LLM\n",
    "            self._init_llm(**llm_kwargs)\n",
    "\n",
    "            # Step 4: Call model\n",
    "            answer = self.llm.generate(prompt)\n",
    "\n",
    "            # Step 5: Return structured output\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"context_length\": len(context),\n",
    "                \"model_type\": self.model_type,\n",
    "                \"success\": True,\n",
    "                \"error\": None,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Xin lỗi, có lỗi xảy ra: {str(e)}\",\n",
    "                \"context_length\": 0,\n",
    "                \"model_type\": self.model_type,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "\n",
    "# Global instance with default model\n",
    "_rag_llm_caller = None\n",
    "\n",
    "\n",
    "def get_rag_llm_caller(model_type: str = \"gemini\", **kwargs) -> RAGLLMCaller:\n",
    "    \"\"\"\n",
    "    Get global RAG LLM caller instance.\n",
    "\n",
    "    Args:\n",
    "        model_type: Type of model to use\n",
    "        **kwargs: Model parameters\n",
    "\n",
    "    Returns:\n",
    "        RAGLLMCaller instance\n",
    "    \"\"\"\n",
    "    global _rag_llm_caller\n",
    "    if _rag_llm_caller is None or _rag_llm_caller.model_type != model_type:\n",
    "        _rag_llm_caller = RAGLLMCaller(model_type=model_type, **kwargs)\n",
    "    return _rag_llm_caller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dcac6",
   "metadata": {},
   "source": [
    "### 3.2 Load retriever and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retrieval_models() -> Tuple[Any, Any, Any]:\n",
    "    \"\"\"\n",
    "    Load and initialize all models needed for RAG retrieval with PyMilvus.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing: Milvus client, embedding model, and reranker model\n",
    "    \"\"\"\n",
    "    # Clear memory before loading models\n",
    "    gc.collect()\n",
    "\n",
    "    # Load BGE-M3 embedding model\n",
    "    embedding_model = BGEM3Encoder(\n",
    "        model=EMBED_MODEL_ID,\n",
    "        device=\"cpu\",  # Force CPU to avoid memory conflicts\n",
    "        normalize_embeddings=True,\n",
    "        use_fp16=False,  # Set to False to avoid dtype issues\n",
    "        max_length=512,\n",
    "        batch_size=16,  # Smaller batch size for memory efficiency\n",
    "    )\n",
    "\n",
    "    # Test embedding model with minimal example\n",
    "    test_result = embedding_model.encode(\"test\")\n",
    "\n",
    "    # Clean up test embedding immediately\n",
    "    del test_result\n",
    "    gc.collect()\n",
    "\n",
    "    # Initialize Milvus client\n",
    "    client = MilvusClient(uri=str(MILVUS_URI))\n",
    "\n",
    "    # Test connection\n",
    "    if client.has_collection(COLLECTION_NAME):\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    # Load BGE reranker model\n",
    "    reranker_model = FlagReranker(\n",
    "        RERANKER_MODEL_ID,\n",
    "        normalize=True,\n",
    "        use_fp16=False,  # Disable FP16 to avoid memory issues\n",
    "        device=\"cpu\",  # Force CPU to avoid CUDA OOM\n",
    "    )\n",
    "\n",
    "    # Final memory cleanup\n",
    "    gc.collect()\n",
    "\n",
    "    return client, embedding_model, reranker_model\n",
    "\n",
    "\n",
    "def get_embedding_model():\n",
    "    \"\"\"Load and return BGE-M3 embedding model.\"\"\"\n",
    "\n",
    "    # Clear memory before loading models\n",
    "    gc.collect()\n",
    "\n",
    "    embedding_model = BGEM3Encoder(\n",
    "        model=EMBED_MODEL_ID,\n",
    "        device=\"cpu\",  # Force CPU to avoid memory conflicts\n",
    "        normalize_embeddings=True,\n",
    "        use_fp16=False,  # Set to False to avoid dtype issues\n",
    "        max_length=512,\n",
    "        batch_size=16,  # Smaller batch size for memory efficiency\n",
    "    )\n",
    "\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "def get_reranker_model():\n",
    "    \"\"\"Load and return BGE reranker model.\"\"\"\n",
    "\n",
    "    reranker_model = FlagReranker(\n",
    "        RERANKER_MODEL_ID,\n",
    "        normalize=True,\n",
    "        use_fp16=False,  # Disable FP16 to avoid memory issues\n",
    "        device=\"cpu\",  # Force CPU to avoid CUDA OOM\n",
    "    )\n",
    "\n",
    "    return reranker_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe84a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    \"\"\"Internal subclass for document retrieval operations.\"\"\"\n",
    "\n",
    "    def __init__(self, parent_rag):\n",
    "        \"\"\"Initialize retriever with reference to parent RAG system.\"\"\"\n",
    "        self.parent = parent_rag\n",
    "\n",
    "    def retrieve_and_rerank(self, query: str) -> Tuple[List[Document], List[float]]:\n",
    "        \"\"\"\n",
    "        Retrieve and rerank documents for a query.\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (documents, rerank_scores)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embeddings first\n",
    "            embeddings = self.parent.embedding_model.encode([query])\n",
    "            query_embedding = embeddings[\"dense_vecs\"][0]\n",
    "            query_sparse = embeddings[\"lexical_weights\"][0]\n",
    "\n",
    "            # Perform hybrid search with embeddings\n",
    "            search_results = hybrid_search(\n",
    "                client=self.parent.client,\n",
    "                collection_name=self.parent.collection_name,\n",
    "                query_embedding=query_embedding,\n",
    "                query_sparse=query_sparse,\n",
    "                k=self.parent.k,\n",
    "                similarity_threshold=self.parent.similarity_threshold,\n",
    "            )\n",
    "\n",
    "            # Convert search results to documents and perform reranking\n",
    "            documents = []\n",
    "            scores = []\n",
    "            for result in search_results:\n",
    "                # Extract text and metadata from entity field\n",
    "                entity = result.get(\"entity\", {})\n",
    "                text_content = entity.get(\"text\", \"\")\n",
    "                metadata = entity.get(\"metadata\", {})\n",
    "\n",
    "                doc = Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata=metadata,\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                # Use the combined score from RRF or the available score\n",
    "                score = result.get(\n",
    "                    \"rrf_score\",\n",
    "                    result.get(\"combined_score\", result.get(\"dense_score\", 0.0)),\n",
    "                )\n",
    "                scores.append(score)\n",
    "\n",
    "            # Perform reranking if we have documents and a reranker model\n",
    "            if documents and self.parent.reranker_model:\n",
    "                try:\n",
    "                    # Prepare texts for reranking\n",
    "                    texts = [doc.page_content for doc in documents]\n",
    "\n",
    "                    # Rerank documents\n",
    "                    rerank_results = self.parent.reranker_model.compute_score(\n",
    "                        [[query, text] for text in texts]\n",
    "                    )\n",
    "\n",
    "                    # Get rerank scores and sort\n",
    "                    if isinstance(rerank_results, list):\n",
    "                        rerank_scores = rerank_results\n",
    "                    else:\n",
    "                        rerank_scores = rerank_results.tolist()\n",
    "\n",
    "                    # Sort by rerank scores and take top k\n",
    "                    scored_docs = list(zip(documents, rerank_scores))\n",
    "                    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                    # Take top rerank_top_k results\n",
    "                    top_k = min(len(scored_docs), self.parent.rerank_top_k)\n",
    "                    top_docs = scored_docs[:top_k]\n",
    "\n",
    "                    documents = [doc for doc, _ in top_docs]\n",
    "                    rerank_scores = [score for _, score in top_docs]\n",
    "\n",
    "                except Exception as e:\n",
    "                    rerank_scores = scores[: self.parent.rerank_top_k]\n",
    "                    documents = documents[: self.parent.rerank_top_k]\n",
    "            else:\n",
    "                # No reranking, just take top results\n",
    "                top_k = min(len(documents), self.parent.rerank_top_k)\n",
    "                documents = documents[:top_k]\n",
    "                rerank_scores = scores[:top_k]\n",
    "\n",
    "            return documents, rerank_scores\n",
    "\n",
    "        except Exception as e:\n",
    "            return [], []\n",
    "\n",
    "\n",
    "class AnswerGenerator:\n",
    "    \"\"\"Internal subclass for answer generation operations.\"\"\"\n",
    "\n",
    "    def __init__(self, parent_rag):\n",
    "        \"\"\"Initialize generator with reference to parent RAG system.\"\"\"\n",
    "        self.parent = parent_rag\n",
    "\n",
    "    def generate_answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "        rerank_scores: Optional[List[float]] = None,\n",
    "        **llm_kwargs,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer from query and retrieved documents.\n",
    "\n",
    "        Args:\n",
    "            query: User question\n",
    "            documents: Retrieved documents\n",
    "            rerank_scores: Reranking scores\n",
    "            **llm_kwargs: Additional LLM parameters\n",
    "\n",
    "        Returns:\n",
    "            Dict with answer, sources, confidence, etc.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"answer\": \"Tôi không tìm thấy thông tin phù hợp để trả lời câu hỏi của bạn.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"success\": False,\n",
    "                \"retrieval_count\": 0,\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Generate answer using LLM\n",
    "            result = self.parent.llm_caller.generate_answer(\n",
    "                query=query,\n",
    "                documents=documents,\n",
    "                rerank_scores=rerank_scores,\n",
    "                **llm_kwargs,\n",
    "            )\n",
    "\n",
    "            # Add retrieval metadata\n",
    "            result[\"retrieval_count\"] = len(documents)\n",
    "            result[\"success\"] = True\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": \"Xin lỗi, đã có lỗi xảy ra khi xử lý câu hỏi của bạn.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"retrieval_count\": len(documents),\n",
    "            }\n",
    "\n",
    "    def switch_model(self, model_type: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Switch LLM model.\n",
    "\n",
    "        Args:\n",
    "            model_type: New model type\n",
    "            **model_kwargs: Model parameters\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create new LLM caller with specified model\n",
    "            self.parent.llm_caller = get_rag_llm_caller(\n",
    "                model_type=model_type, **model_kwargs\n",
    "            )\n",
    "\n",
    "            # Update parent's model type tracking\n",
    "            self.parent._current_model_type = model_type\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "\n",
    "class VietnameseRAG:\n",
    "    \"\"\"\n",
    "    Unified Vietnamese RAG system with modular subclasses.\n",
    "\n",
    "    This class provides the complete RAG flow while using internal\n",
    "    subclasses for clean modular architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Connection parameters\n",
    "        client=None,\n",
    "        collection_name: str = COLLECTION_NAME,\n",
    "        # Model parameters\n",
    "        embedding_model=None,\n",
    "        reranker_model=None,\n",
    "        # Retrieval parameters\n",
    "        k: int = DEFAULT_K,\n",
    "        rerank_top_k: int = RERANK_TOP_K,\n",
    "        similarity_threshold: float = SIMILARITY_THRESHOLD,\n",
    "        # LLM parameters\n",
    "        model_type: str = \"gemini\",\n",
    "        llm_caller: Optional[RAGLLMCaller] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize unified Vietnamese RAG system.\n",
    "\n",
    "        Args:\n",
    "            client: Milvus client\n",
    "            collection_name: Collection name in Milvus\n",
    "            embedding_model: BGE-M3 embedding model\n",
    "            reranker_model: Reranker model\n",
    "            k: Initial retrieval count\n",
    "            rerank_top_k: Final count after reranking\n",
    "            similarity_threshold: Minimum similarity threshold\n",
    "            model_type: LLM model type (gemini, watsonx, etc.)\n",
    "            llm_caller: Custom LLM caller instance\n",
    "            **llm_kwargs: Additional LLM parameters\n",
    "        \"\"\"\n",
    "        # Store configuration\n",
    "        self.collection_name = collection_name\n",
    "        self.k = k\n",
    "        self.rerank_top_k = rerank_top_k\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self._current_model_type = model_type\n",
    "\n",
    "        # Initialize Milvus client\n",
    "        self.client = client or get_milvus_client()\n",
    "\n",
    "        # Load models\n",
    "        self.embedding_model = embedding_model or get_embedding_model()\n",
    "        self.reranker_model = reranker_model or get_reranker_model()\n",
    "\n",
    "        # Initialize LLM caller\n",
    "        self.llm_caller = llm_caller or get_rag_llm_caller(\n",
    "            model_type=model_type, **llm_kwargs\n",
    "        )\n",
    "\n",
    "        # Initialize modular subclasses\n",
    "        self.retriever = DocumentRetriever(self)\n",
    "        self.generator = AnswerGenerator(self)\n",
    "\n",
    "    def answer(self, query: str, **llm_kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Main method: Complete RAG flow from query to answer.\n",
    "\n",
    "        Args:\n",
    "            query: User question\n",
    "            **llm_kwargs: Additional LLM parameters\n",
    "\n",
    "        Returns:\n",
    "            Complete result with answer, sources, confidence, etc.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Retrieve and rerank documents\n",
    "            documents, rerank_scores = self.retriever.retrieve_and_rerank(query)\n",
    "\n",
    "            if not documents:\n",
    "                return {\n",
    "                    \"answer\": \"Tôi không tìm thấy thông tin phù hợp để trả lời câu hỏi của bạn.\",\n",
    "                    \"sources\": [],\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"success\": False,\n",
    "                    \"retrieval_count\": 0,\n",
    "                }\n",
    "\n",
    "            # Step 2: Generate answer\n",
    "            result = self.generator.generate_answer(\n",
    "                query=query,\n",
    "                documents=documents,\n",
    "                rerank_scores=rerank_scores,\n",
    "                **llm_kwargs,\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": \"Xin lỗi, đã có lỗi xảy ra khi xử lý câu hỏi của bạn.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"retrieval_count\": 0,\n",
    "            }\n",
    "\n",
    "    def switch_model(self, model_type: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Switch LLM model.\n",
    "\n",
    "        Args:\n",
    "            model_type: New model type (gemini, watsonx, etc.)\n",
    "            **model_kwargs: Model-specific parameters\n",
    "        \"\"\"\n",
    "        self.generator.switch_model(model_type, **model_kwargs)\n",
    "\n",
    "    @property\n",
    "    def model_type(self) -> str:\n",
    "        \"\"\"Get current LLM model type.\"\"\"\n",
    "        return self._current_model_type\n",
    "\n",
    "    @property\n",
    "    def status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system status and configuration.\"\"\"\n",
    "        return {\n",
    "            \"model_type\": self.model_type,\n",
    "            \"collection_name\": self.collection_name,\n",
    "            \"retrieval_config\": {\n",
    "                \"k\": self.k,\n",
    "                \"rerank_top_k\": self.rerank_top_k,\n",
    "                \"similarity_threshold\": self.similarity_threshold,\n",
    "            },\n",
    "            \"models_loaded\": {\n",
    "                \"embedding\": self.embedding_model is not None,\n",
    "                \"reranker\": self.reranker_model is not None,\n",
    "                \"llm\": self.llm_caller is not None,\n",
    "            },\n",
    "            \"milvus_connected\": self.client is not None,\n",
    "        }\n",
    "\n",
    "    def update_config(\n",
    "        self,\n",
    "        k: Optional[int] = None,\n",
    "        rerank_top_k: Optional[int] = None,\n",
    "        similarity_threshold: Optional[float] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update retrieval configuration.\n",
    "\n",
    "        Args:\n",
    "            k: New initial retrieval count\n",
    "            rerank_top_k: New reranking count\n",
    "            similarity_threshold: New similarity threshold\n",
    "        \"\"\"\n",
    "        if k is not None:\n",
    "            self.k = k\n",
    "\n",
    "        if rerank_top_k is not None:\n",
    "            self.rerank_top_k = rerank_top_k\n",
    "\n",
    "        if similarity_threshold is not None:\n",
    "            self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "# Convenience function for backward compatibility\n",
    "def get_vietnamese_rag(**kwargs) -> VietnameseRAG:\n",
    "    \"\"\"\n",
    "    Get Vietnamese RAG instance with default configuration.\n",
    "\n",
    "    Args:\n",
    "        **kwargs: Configuration parameters\n",
    "\n",
    "    Returns:\n",
    "        VietnameseRAG instance\n",
    "    \"\"\"\n",
    "    return VietnameseRAG(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1742f49",
   "metadata": {},
   "source": [
    "### 3.3 Define RAG Chain and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_retrieval(rag, query: str) -> Tuple[List[Document], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform document retrieval and reranking only.\n",
    "\n",
    "    Args:\n",
    "        rag: The Vietnamese RAG system instance\n",
    "        query: The search query\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (documents, rerank_scores)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the retriever directly for cleaner separation\n",
    "        documents, scores = rag.retriever.retrieve_and_rerank(query)\n",
    "\n",
    "        return documents, scores\n",
    "\n",
    "    except Exception as e:\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def generate_answer(rag, query: str, documents: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using LLM based on retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        rag: The Vietnamese RAG system instance\n",
    "        query: The search query\n",
    "        documents: Retrieved documents\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing answer and metadata\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"No documents provided for answer generation\",\n",
    "            \"answer\": None,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Use the answer generator directly\n",
    "        answer_result = rag.answer_generator.generate_answer(query, documents)\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"answer\": answer_result.get(\"answer\", \"\"),\n",
    "            \"confidence\": answer_result.get(\"confidence\", 0.0),\n",
    "            \"error\": None,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"answer\": None}\n",
    "\n",
    "\n",
    "def display_results(\n",
    "    query: str,\n",
    "    documents: List[Document],\n",
    "    scores: List[float],\n",
    "    answer_result: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display the retrieval and answer generation results.\n",
    "\n",
    "    Args:\n",
    "        query: The search query\n",
    "        documents: Retrieved documents\n",
    "        scores: Document scores\n",
    "        answer_result: LLM answer generation result\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        print(\"❌ No relevant documents found.\")\n",
    "        return\n",
    "\n",
    "    # If answer generation succeeded, show only query and answer\n",
    "    if answer_result.get(\"success\") and answer_result.get(\"answer\"):\n",
    "        print(f\"\\n🔍 **Query:** {query}\")\n",
    "        print(f\"\\n🤖 **Answer:**\")\n",
    "        print(\"=\" * 50)\n",
    "        print(answer_result[\"answer\"])\n",
    "\n",
    "        # Log the successful Q&A pair\n",
    "        retrieval_info = {\n",
    "            \"num_results\": len(documents),\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"filename\": doc.metadata.get(\"source_filename\", \"Unknown\"),\n",
    "                    \"score\": score,\n",
    "                    \"content_length\": len(doc.page_content),\n",
    "                }\n",
    "                for doc, score in zip(documents, scores)\n",
    "            ],\n",
    "            \"confidence\": answer_result.get(\"confidence\", 0.0),\n",
    "        }\n",
    "\n",
    "    # If answer generation failed, show LLM calling prompt\n",
    "    else:\n",
    "        print(f\"\\n⚠️ **LLM calling failed or returned no answer**\")\n",
    "        if answer_result.get(\"error\"):\n",
    "            print(f\"Error: {answer_result['error']}\")\n",
    "\n",
    "        print(f\"\\n🔍 Query: {query}\")\n",
    "        print(\n",
    "            f\"📊 Retrieved {len(documents)} documents but LLM answer generation failed.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00694011",
   "metadata": {},
   "outputs": [],
   "source": [
    "client, supports_hnsw = get_milvus_client()\n",
    "embedding_model, reranker_model = load_retrieval_models()\n",
    "rag = VietnameseRAG(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_model=embedding_model,\n",
    "    reranker_model=reranker_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n💡 Enter 'quit' to exit the system\")\n",
    "print(\"🤖 Complete RAG: BGE-M3 search → LLM answer generation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get user input\n",
    "        query = input(\"\\n🔍 Enter your query: \").strip()\n",
    "\n",
    "        # Check for quit command\n",
    "        if query.lower() in [\"quit\", \"q\", \"exit\"]:\n",
    "            print(\"👋 Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if not query:\n",
    "            print(\"⚠️ Please enter a valid query\")\n",
    "            continue\n",
    "\n",
    "        # Step 1: Perform retrieval\n",
    "        documents, scores = perform_retrieval(rag, query)\n",
    "\n",
    "        # Step 2: Generate answer (if documents found)\n",
    "        answer_result = generate_answer(rag, query, documents)\n",
    "\n",
    "        # Step 3: Display results\n",
    "        display_results(query, documents, scores, answer_result)\n",
    "\n",
    "        # Clean up after each query\n",
    "        gc.collect()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n👋 Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing query: {e}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
