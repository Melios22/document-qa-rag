{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fa8581",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "### [0. Global Vars](#0-global-vars)\n",
    "Configuration constants and system parameters for all three RAG stages\n",
    "\n",
    "### [1. Process PDFs](#1-process-pdfs)\n",
    "Document preprocessing pipeline with Vietnamese text optimization\n",
    "- [1.1 Text Simple Cleaner and Metadata](#11-text-simple-cleaner-and-metadata)\n",
    "- [1.2 Run PDF Processing Pipeline](#12-run-pdf-processing-pipeline)\n",
    "\n",
    "### [2. Build RAG](#2-build-rag)\n",
    "Vector database construction with hybrid BGE-M3 embeddings\n",
    "- [2.1 Milvus Client Connection](#21-milvus-client-connection)\n",
    "- [2.2 Milvus Builder and Search](#22-milvus-builder-and-search)\n",
    "- [2.3 Embedding Model and Builder Vector Store](#23-embedding-model-and-builder-vector-store)\n",
    "\n",
    "### [3. Retrieve from RAG](#3-retrieve-from-rag)\n",
    "Interactive retrieval system with LLM answer generation\n",
    "- [3.1 LLM Definition and Call Managements](#31-llm-definition-and-call-managements)\n",
    "- [3.2 Load retriever and models](#32-load-retriever-and-models)\n",
    "- [3.3 Define RAG Chain and Run](#33-define-rag-chain-and-run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd702b",
   "metadata": {},
   "source": [
    "___\n",
    "## 0. Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d993e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "CHARS_PER_TOKEN = 3\n",
    "CHUNKER_MODEL = \"BAAI/bge-m3\"\n",
    "MAX_CHUNK_TOKENS = 1024\n",
    "OVERLAP_TOKENS = 128\n",
    "\n",
    "# Build RAG\n",
    "DENSE_INDEX_CONFIG = {\n",
    "    \"index_type\": \"HNSW\",\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"params\": {\"M\": 16, \"efConstruction\": 64},\n",
    "}\n",
    "DENSE_INDEX_FALLBACK_CONFIG = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"params\": {\"nlist\": 256},\n",
    "}\n",
    "DENSE_SEARCH_FALLBACK_PARAMS = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 8}}\n",
    "DENSE_SEARCH_PARAMS = {\"metric_type\": \"IP\", \"params\": {\"drop_ratio_search\": 0.2}}\n",
    "MILVUS_DOCKER_URI = \"http://localhost:19530\"\n",
    "MILVUS_URI = \"data/milvus.db\"\n",
    "USE_DOCKER_MILVUS = False\n",
    "\n",
    "RRF_K = 30\n",
    "SPARSE_INDEX_CONFIG = {\n",
    "    \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "    \"metric_type\": \"IP\",\n",
    "    \"params\": {\"drop_ratio_build\": 0.2},\n",
    "}\n",
    "SPARSE_SEARCH_PARAMS = {\"metric_type\": \"IP\", \"params\": {\"drop_ratio_search\": 0.2}}\n",
    "\n",
    "COLLECTION_NAME = \"vndoc_rag_hybrid\"\n",
    "EMBED_MODEL_ID = CHUNKER_MODEL\n",
    "EMBEDDING_DIM = 1024\n",
    "ENCODE_KWARGS = {\n",
    "    \"normalize_embeddings\": True,\n",
    "    \"batch_size\": 8,\n",
    "    \"return_dense\": True,\n",
    "    \"return_sparse\": True,\n",
    "    \"return_colbert_vecs\": False,\n",
    "}\n",
    "\n",
    "# Retrieve\n",
    "MAX_OUTPUT_TOKENS = 4096\n",
    "TEMPERATURE = 0\n",
    "PROMPT = \"\"\"Báº¡n lÃ  má»™t chuyÃªn gia trÃ­ tuá»‡ nhÃ¢n táº¡o vÃ  há»c mÃ¡y cÃ³ kiáº¿n thá»©c sÃ¢u rá»™ng. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p tá»« há»‡ thá»‘ng RAG.\\n\\nCÃ‚U Há»ŽI: {query}\\n\\nTHÃ”NG TIN THAM KHáº¢O:\\n{context}\\n\\nHÆ¯á»šNG DáºªN TRáº¢ Lá»œI:\\n1. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch chi tiáº¿t vÃ  rÃµ rÃ ng\\n2. Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p - KHÃ”NG tá»± bá»‹a Ä‘áº·t hoáº·c thÃªm thÃ´ng tin\\n3. Má»—i nguá»“n tÃ i liá»‡u lÃ  riÃªng biá»‡t - KHÃ”NG trá»™n láº«n hoáº·c káº¿t há»£p thÃ´ng tin tá»« cÃ¡c nguá»“n khÃ¡c nhau má»™t cÃ¡ch tÃ¹y tiá»‡n\\n4. Náº¿u thÃ´ng tin tá»« cÃ¡c nguá»“n khÃ¡c nhau mÃ¢u thuáº«n, hÃ£y chá»‰ ra sá»± khÃ¡c biá»‡t nÃ y\\n5. Giáº£i thÃ­ch cÃ¡c thuáº­t ngá»¯ ká»¹ thuáº­t báº±ng tiáº¿ng Viá»‡t\\n6. TrÃ­ch dáº«n rÃµ rÃ ng nguá»“n thÃ´ng tin cho má»—i pháº§n tráº£ lá»i\\n7. Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§ Ä‘á»ƒ tráº£ lá»i Ä‘áº§y Ä‘á»§, hÃ£y thá»«a nháº­n Ä‘iá»u nÃ y thay vÃ¬ Ä‘oÃ¡n\"\"\"\n",
    "\n",
    "DEFAULT_K = 10\n",
    "RERANK_TOP_K = 3\n",
    "SIMILARITY_THRESHOLD = 0.2\n",
    "RERANKER_MODEL_ID = \"BAAI/bge-reranker-v2-m3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab83e4",
   "metadata": {},
   "source": [
    "## 1. Process PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain.schema import Document\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "import hashlib\n",
    "import re\n",
    "import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b531619f",
   "metadata": {},
   "source": [
    "### 1.1 Text Simple Cleaner and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    \"\"\"Vietnamese text cleaner optimized for RAG preprocessing.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Allow Vietnamese letters + digits + punctuation\n",
    "        self.vietnamese_chars = (\n",
    "            \"Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµ\"\n",
    "            \"Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…\"\n",
    "            \"Ã¬Ã­á»‹á»‰Ä©\"\n",
    "            \"Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡\"\n",
    "            \"Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯\"\n",
    "            \"á»³Ã½á»µá»·á»¹\"\n",
    "            \"Ä‘\"\n",
    "            \"Ã€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´\"\n",
    "            \"ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„\"\n",
    "            \"ÃŒÃá»Šá»ˆÄ¨\"\n",
    "            \"Ã’Ã“á»Œá»ŽÃ•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»žá» \"\n",
    "            \"Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®\"\n",
    "            \"á»²Ãá»´á»¶á»¸\"\n",
    "            \"Ä\"\n",
    "        )\n",
    "\n",
    "    def normalize_unicode(self, text: str) -> str:\n",
    "        \"\"\"Normalize Unicode (NFC) for consistency.\"\"\"\n",
    "        return unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    def clean_text(self, text: str) -> tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"Clean text and return metadata for RAG preparation.\"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return \"\", {\"was_empty\": True}\n",
    "\n",
    "        gc.collect()\n",
    "        original_length = len(text)\n",
    "\n",
    "        # Unicode normalization\n",
    "        text = self.normalize_unicode(text)\n",
    "\n",
    "        # Basic noise cleanup\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)  # collapse spaces/tabs\n",
    "        text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)  # normalize paragraph breaks\n",
    "        text = re.sub(r\"[.]{3,}\", \"...\", text)  # reduce ellipses\n",
    "        text = re.sub(r\"[-]{3,}\", \"â€”\", text)  # convert long dashes\n",
    "        text = re.sub(r\"[,]{2,}\", \",\", text)  # collapse commas\n",
    "\n",
    "        # Remove stray non-text characters (OCR artifacts, control chars)\n",
    "        vietnamese_escaped = re.escape(self.vietnamese_chars)\n",
    "        allowed_pattern = f\"[a-zA-Z0-9{vietnamese_escaped}\\\\s.,;:!?()\\\\[\\\\]{{}}\\\"'\\\\-_/\\\\\\\\+=%&@#$\\\\n\\\\t]\"\n",
    "        text = \"\".join(ch for ch in text if re.match(allowed_pattern, ch))\n",
    "\n",
    "        # Normalize line breaks around punctuation (fix OCR issues)\n",
    "        text = re.sub(r\"\\s*([.,;:!?])\\s*\", r\"\\1 \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        cleaned_text = text.strip()\n",
    "\n",
    "        # Metadata for tracking\n",
    "        cleaning_metadata = {\n",
    "            \"was_empty\": False,\n",
    "            \"original_length\": original_length,\n",
    "            \"cleaned_length\": len(cleaned_text),\n",
    "            \"word_count\": len(cleaned_text.split()),\n",
    "            \"paragraphs\": cleaned_text.count(\"\\n\\n\") + 1,\n",
    "            \"estimated_tokens\": self.estimate_tokens(cleaned_text),\n",
    "        }\n",
    "\n",
    "        return cleaned_text, cleaning_metadata\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Rough token estimation (tune per tokenizer).\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "        return len(text) // CHARS_PER_TOKEN + 1\n",
    "\n",
    "\n",
    "class ChunkMetadataGenerator:\n",
    "    \"\"\"Generates core metadata for chunks\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session_id = str(uuid.uuid4())[:8]\n",
    "        self.chunk_counter = 0\n",
    "\n",
    "    def generate_chunk_id(self, source_file: str, chunk_index: int) -> str:\n",
    "        \"\"\"Generate unique chunk ID\"\"\"\n",
    "        file_hash = hashlib.md5(source_file.encode()).hexdigest()[:8]\n",
    "        return f\"CHUNK_{self.session_id}_{file_hash}_{chunk_index:04d}\"\n",
    "\n",
    "    def generate_document_id(self, source_file: str) -> str:\n",
    "        \"\"\"Generate unique document ID\"\"\"\n",
    "        file_hash = hashlib.md5(source_file.encode()).hexdigest()[:8]\n",
    "        return f\"DOC_{self.session_id}_{file_hash}\"\n",
    "\n",
    "    def create_metadata(\n",
    "        self,\n",
    "        doc: Document,\n",
    "        source_file: str,\n",
    "        chunk_index: int,\n",
    "        cleaning_metadata: Dict[str, Any],\n",
    "        estimated_tokens: int,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create chunk metadata\"\"\"\n",
    "        gc.collect()\n",
    "        self.chunk_counter += 1\n",
    "\n",
    "        # Generate IDs\n",
    "        chunk_id = self.generate_chunk_id(source_file, chunk_index)\n",
    "        document_id = self.generate_document_id(source_file)\n",
    "\n",
    "        # Extract file information\n",
    "        file_path = Path(source_file)\n",
    "        content = doc.page_content\n",
    "        content_hash = hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "        # Core metadata\n",
    "        metadata = {\n",
    "            # Essential IDs\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"document_id\": document_id,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"global_index\": self.chunk_counter,\n",
    "            # Processing info\n",
    "            \"session_id\": self.session_id,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"chunker_model\": CHUNKER_MODEL,\n",
    "            # Source info\n",
    "            \"source_file\": source_file,\n",
    "            \"source_filename\": str(file_path.name),\n",
    "            # Content metrics\n",
    "            \"content_length\": len(content),\n",
    "            \"estimated_tokens\": estimated_tokens,\n",
    "            \"word_count\": cleaning_metadata.get(\"word_count\", 0),\n",
    "            \"content_hash\": content_hash,\n",
    "            # # Future use placeholders\n",
    "            # \"summary\": None,\n",
    "            # \"keywords\": None,\n",
    "            # \"embedding_id\": None,\n",
    "            # \"last_accessed\": None,\n",
    "        }\n",
    "\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab95c88",
   "metadata": {},
   "source": [
    "### 1.2 Run PDF Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc57bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleaner()\n",
    "metadata_generator = ChunkMetadataGenerator()\n",
    "\n",
    "pdf_files: List[str] = []\n",
    "all_documents: List[Document] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf914d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = HybridChunker(\n",
    "    tokenizer=CHUNKER_MODEL,\n",
    "    max_tokens=MAX_CHUNK_TOKENS,\n",
    "    overlap_tokens=OVERLAP_TOKENS,\n",
    ")\n",
    "\n",
    "# Process each PDF file\n",
    "pbar = tqdm(total=len(pdf_files), desc=\"Processing PDFs\")\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pbar.set_description(f\"Processing {pdf_file.name}\")\n",
    "    pdf_file_str = str(pdf_file)\n",
    "\n",
    "    try:\n",
    "        # Use DoclingLoader with chunker\n",
    "        loader = DoclingLoader(\n",
    "            file_path=pdf_file_str,\n",
    "            export_type=ExportType.DOC_CHUNKS,\n",
    "            chunker=chunker,\n",
    "        )\n",
    "\n",
    "        # Load documents - already chunked\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Process each chunk\n",
    "        file_chunks_processed = 0\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            # Clean text\n",
    "            cleaned_content, cleaning_metadata = text_cleaner.clean_text(\n",
    "                doc.page_content\n",
    "            )\n",
    "\n",
    "            if cleaned_content and len(cleaned_content) > 50:\n",
    "                estimated_tokens = text_cleaner.estimate_tokens(cleaned_content)\n",
    "\n",
    "                # Create metadata\n",
    "                metadata = metadata_generator.create_metadata(\n",
    "                    doc=doc,\n",
    "                    source_file=pdf_file_str,\n",
    "                    chunk_index=i,\n",
    "                    cleaning_metadata=cleaning_metadata,\n",
    "                    estimated_tokens=estimated_tokens,\n",
    "                )\n",
    "\n",
    "                # Update document with cleaned content and metadata\n",
    "                doc.page_content = cleaned_content\n",
    "                doc.metadata = metadata\n",
    "\n",
    "                all_documents.append(doc)\n",
    "                file_chunks_processed += 1\n",
    "\n",
    "        del loader\n",
    "        pbar.update(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "del chunker\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81680ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return out: `all_documents`\n",
    "display(all_documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dec205",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Build RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import Union\n",
    "from pymilvus import (\n",
    "    AnnSearchRequest,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    FieldSchema,\n",
    "    MilvusClient,\n",
    "    RRFRanker,\n",
    "    connections,\n",
    ")\n",
    "from pymilvus.milvus_client.index import IndexParams\n",
    "from FlagEmbedding import BGEM3FlagModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0570b8",
   "metadata": {},
   "source": [
    "### 2.1 Milvus Client Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e415832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusConnectionManager:\n",
    "    \"\"\"Manages Milvus connections with Docker support\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client: Optional[MilvusClient] = None\n",
    "        self.supports_hnsw = False\n",
    "\n",
    "    def get_client(self) -> Tuple[MilvusClient, bool]:\n",
    "        \"\"\"Get Milvus client with automatic connection management\"\"\"\n",
    "        if self.client is not None:\n",
    "            return self.client, self.supports_hnsw\n",
    "\n",
    "        # Try Docker connection first if enabled\n",
    "        if USE_DOCKER_MILVUS:\n",
    "            try:\n",
    "                print(\"ðŸ³ Connecting to Docker Milvus...\")\n",
    "                self.client = MilvusClient(uri=MILVUS_DOCKER_URI)\n",
    "                # Test connection\n",
    "                self.client.list_collections()\n",
    "                self.supports_hnsw = True\n",
    "                print(\"âœ… Docker Milvus connected (HNSW enabled)\")\n",
    "                return self.client, True\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Docker connection failed: {e}\")\n",
    "\n",
    "        # Fallback to local file-based connection\n",
    "        print(\"ðŸ“ Using local file-based Milvus...\")\n",
    "        self.client = MilvusClient(uri=str(MILVUS_URI))\n",
    "        self.supports_hnsw = False\n",
    "        print(\"âœ… Local Milvus connected\")\n",
    "        return self.client, False\n",
    "\n",
    "    def get_index_config(self):\n",
    "        \"\"\"Get appropriate index configuration\"\"\"\n",
    "        if self.supports_hnsw:\n",
    "            return DENSE_INDEX_CONFIG, DENSE_SEARCH_PARAMS\n",
    "        else:\n",
    "            return DENSE_INDEX_FALLBACK_CONFIG, DENSE_SEARCH_FALLBACK_PARAMS\n",
    "\n",
    "\n",
    "# Global connection manager\n",
    "_connection_manager = MilvusConnectionManager()\n",
    "\n",
    "\n",
    "def get_milvus_client() -> Tuple[MilvusClient, bool]:\n",
    "    \"\"\"Get the global Milvus client instance\"\"\"\n",
    "    return _connection_manager.get_client()\n",
    "\n",
    "\n",
    "def get_index_config():\n",
    "    \"\"\"Get appropriate index configuration for current connection\"\"\"\n",
    "    return _connection_manager.get_index_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eea7cd",
   "metadata": {},
   "source": [
    "### 2.2 Milvus Builder and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d82fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_hybrid_collection(client: MilvusClient, name: str, dense_dim: int) -> bool:\n",
    "    \"\"\"\n",
    "    Ensure hybrid collection exists with proper schema.\n",
    "\n",
    "    Returns:\n",
    "        True if collection was created/recreated, False if it already existed\n",
    "    \"\"\"\n",
    "    if client.has_collection(name):\n",
    "        client.drop_collection(name)\n",
    "\n",
    "    schema = CollectionSchema(\n",
    "        fields=[\n",
    "            FieldSchema(\n",
    "                name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=64\n",
    "            ),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "            FieldSchema(\n",
    "                name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=dense_dim\n",
    "            ),\n",
    "            FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "            FieldSchema(name=\"metadata\", dtype=DataType.JSON),\n",
    "        ],\n",
    "        description=\"Hybrid dense+sparse collection for BGE-M3\",\n",
    "    )\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=name, schema=schema, consistency_level=\"Strong\"\n",
    "    )\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def build_indexes(client: MilvusClient, name: str) -> None:\n",
    "    \"\"\"Build indexes with automatic HNSW/IVF_FLAT selection\"\"\"\n",
    "    try:\n",
    "        # Get appropriate index configuration\n",
    "        dense_index_config, dense_search_params = get_index_config()\n",
    "\n",
    "        # Dense index with auto-detection\n",
    "        dense_index_params = IndexParams()\n",
    "        dense_index_params.add_index(\n",
    "            field_name=\"dense_vector\",\n",
    "            index_type=dense_index_config[\"index_type\"],\n",
    "            metric_type=dense_index_config[\"metric_type\"],\n",
    "            params=dense_index_config[\"params\"],\n",
    "        )\n",
    "        client.create_index(\n",
    "            collection_name=name,\n",
    "            index_params=dense_index_params,\n",
    "        )\n",
    "\n",
    "        # Sparse index using config\n",
    "        sparse_index_params = IndexParams()\n",
    "        sparse_index_params.add_index(\n",
    "            field_name=\"sparse_vector\",\n",
    "            index_type=SPARSE_INDEX_CONFIG[\"index_type\"],\n",
    "            metric_type=SPARSE_INDEX_CONFIG[\"metric_type\"],\n",
    "            params=SPARSE_INDEX_CONFIG[\"params\"],\n",
    "        )\n",
    "        client.create_index(\n",
    "            collection_name=name,\n",
    "            index_params=sparse_index_params,\n",
    "        )\n",
    "        client.load_collection(name)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "\n",
    "def insert_documents(\n",
    "    client: MilvusClient,\n",
    "    name: str,\n",
    "    dense_vecs,\n",
    "    sparse_vecs,\n",
    "    docs: List[Document],\n",
    ") -> None:\n",
    "    \"\"\"Insert documents with better error handling\"\"\"\n",
    "    try:\n",
    "        rows = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"id\": uuid.uuid4().hex,\n",
    "                    \"text\": doc.page_content,\n",
    "                    \"dense_vector\": dense_vecs[i].tolist(),\n",
    "                    \"sparse_vector\": sparse_vecs[i],  # dict {token_id: weight}\n",
    "                    \"metadata\": doc.metadata or {},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        result = client.insert(collection_name=name, data=rows)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    uri: str,\n",
    "    name: str,\n",
    "    dense_q,\n",
    "    sparse_q,\n",
    "    k: int = 20,\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Perform hybrid search with automatic parameter selection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure connection for Collection API\n",
    "        connection_alias = \"default\"\n",
    "        try:\n",
    "            connections.get_connection(alias=connection_alias)\n",
    "        except Exception:\n",
    "            connections.connect(alias=connection_alias, uri=uri)\n",
    "\n",
    "        coll = Collection(name)\n",
    "\n",
    "        # Get appropriate search parameters\n",
    "        _, dense_search_params = get_index_config()\n",
    "\n",
    "        # Create search requests using appropriate parameters\n",
    "        dense_req = AnnSearchRequest(\n",
    "            data=[dense_q],\n",
    "            anns_field=\"dense_vector\",\n",
    "            param=dense_search_params,\n",
    "            limit=k,\n",
    "        )\n",
    "\n",
    "        sparse_req = AnnSearchRequest(\n",
    "            data=[sparse_q],\n",
    "            anns_field=\"sparse_vector\",\n",
    "            param=SPARSE_SEARCH_PARAMS,\n",
    "            limit=k,\n",
    "        )\n",
    "\n",
    "        # Create RRF ranker using config\n",
    "        ranker = RRFRanker(k=RRF_K)\n",
    "\n",
    "        # Perform hybrid search\n",
    "        res = coll.hybrid_search(\n",
    "            reqs=[dense_req, sparse_req],\n",
    "            rerank=ranker,\n",
    "            limit=k,\n",
    "            output_fields=[\"text\", \"metadata\"],\n",
    "        )\n",
    "\n",
    "        # Convert to (Document, score)\n",
    "        out: List[Tuple[Document, float]] = []\n",
    "        for hit in res[0]:\n",
    "            # PyMilvus hit object structure\n",
    "            meta = hit.entity.get(\"metadata\", {})\n",
    "            text = hit.entity.get(\"text\", \"\")\n",
    "            score = float(hit.distance)\n",
    "            out.append((Document(page_content=text, metadata=meta), score))\n",
    "\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in hybrid search: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_dense(\n",
    "    client: MilvusClient,\n",
    "    collection_name: str,\n",
    "    query_embedding: List[float],\n",
    "    k: int = 20,\n",
    "    search_params: Optional[Dict] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform dense vector search.\n",
    "\n",
    "    Args:\n",
    "        client: Milvus client\n",
    "        collection_name: Name of the collection\n",
    "        query_embedding: Dense query embedding\n",
    "        k: Number of results to return\n",
    "        search_params: Search parameters\n",
    "\n",
    "    Returns:\n",
    "        List of search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_params = search_params or DENSE_SEARCH_PARAMS\n",
    "\n",
    "        results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"dense_vector\",\n",
    "            search_params=search_params,\n",
    "            limit=k,\n",
    "            output_fields=[\"text\", \"metadata\"],\n",
    "        )\n",
    "\n",
    "        # Convert to standard format\n",
    "        formatted_results = []\n",
    "        for i, hit in enumerate(results[0]):\n",
    "            formatted_results.append(\n",
    "                {\n",
    "                    \"id\": hit.id,  # Use attribute, not dict access\n",
    "                    \"dense_score\": float(hit.distance),  # Use attribute directly\n",
    "                    \"rank\": i + 1,\n",
    "                    \"entity\": hit.entity,  # hit.entity is already a dict\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def search_sparse(\n",
    "    client: MilvusClient,\n",
    "    collection_name: str,\n",
    "    query_sparse: Dict,\n",
    "    k: int = 20,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform sparse vector search.\n",
    "\n",
    "    Args:\n",
    "        client: Milvus client\n",
    "        collection_name: Name of the collection\n",
    "        query_sparse: Sparse query embedding (dict of token_id -> weight)\n",
    "        k: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        List of search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            data=[query_sparse],\n",
    "            anns_field=\"sparse_vector\",\n",
    "            search_params=SPARSE_SEARCH_PARAMS,\n",
    "            limit=k,\n",
    "            output_fields=[\"text\", \"metadata\"],\n",
    "        )\n",
    "\n",
    "        # Convert to standard format\n",
    "        formatted_results = []\n",
    "        for i, hit in enumerate(results[0]):\n",
    "            formatted_results.append(\n",
    "                {\n",
    "                    \"id\": hit.id,  # Use attribute, not dict access\n",
    "                    \"sparse_score\": float(hit.distance),  # Use attribute directly\n",
    "                    \"rank\": i + 1,\n",
    "                    \"entity\": hit.entity,  # hit.entity is already a dict\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    dense_results: List[Dict],\n",
    "    sparse_results: List[Dict],\n",
    "    k: int = 10,\n",
    "    dense_weight: float = 0.7,\n",
    "    sparse_weight: float = 0.3,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Simplified RRF fusion with better scoring.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc_scores = {}\n",
    "        all_docs = {}\n",
    "\n",
    "        # Process dense results\n",
    "        for rank, result in enumerate(dense_results, 1):\n",
    "            doc_id = result.get(\"id\")\n",
    "            if doc_id:\n",
    "                rrf_score = dense_weight / (k + rank)\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score\n",
    "                all_docs[doc_id] = result\n",
    "                all_docs[doc_id][\"dense_score\"] = result.get(\"dense_score\", 0)\n",
    "\n",
    "        # Process sparse results\n",
    "        for rank, result in enumerate(sparse_results, 1):\n",
    "            doc_id = result.get(\"id\")\n",
    "            if doc_id:\n",
    "                rrf_score = sparse_weight / (k + rank)\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score\n",
    "\n",
    "                if doc_id in all_docs:\n",
    "                    all_docs[doc_id][\"sparse_score\"] = result.get(\"sparse_score\", 0)\n",
    "                else:\n",
    "                    all_docs[doc_id] = result\n",
    "                    all_docs[doc_id][\"sparse_score\"] = result.get(\"sparse_score\", 0)\n",
    "                    all_docs[doc_id][\"dense_score\"] = 0\n",
    "\n",
    "        # Normalize scores to 0-1 range\n",
    "        if doc_scores:\n",
    "            max_score = max(doc_scores.values())\n",
    "            min_score = min(doc_scores.values())\n",
    "            score_range = max_score - min_score\n",
    "\n",
    "            if score_range > 0:\n",
    "                for doc_id in doc_scores:\n",
    "                    normalized = (doc_scores[doc_id] - min_score) / score_range\n",
    "                    doc_scores[doc_id] = 0.1 + 0.9 * normalized\n",
    "            else:\n",
    "                for doc_id in doc_scores:\n",
    "                    doc_scores[doc_id] = 0.5\n",
    "\n",
    "        # Sort and format results\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        fused_results = []\n",
    "        for doc_id, rrf_score in sorted_docs:\n",
    "            doc = all_docs[doc_id].copy()\n",
    "            doc[\"rrf_score\"] = rrf_score\n",
    "            doc[\"combined_score\"] = rrf_score\n",
    "            fused_results.append(doc)\n",
    "\n",
    "        return fused_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in RRF fusion: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9470f",
   "metadata": {},
   "source": [
    "### 2.3 Embedding Model and Builder Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGEM3Encoder:\n",
    "    \"\"\"Encode text with BGE-M3 producing dense and sparse vectors.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"BAAI/bge-m3\",\n",
    "        device: str = \"cpu\",\n",
    "        normalize_embeddings: bool = True,\n",
    "        use_fp16: bool = False,  # Set to False to avoid dtype mismatches\n",
    "        max_length: int = 512,\n",
    "        batch_size: int = 32,\n",
    "        trust_remote_code: bool = True,\n",
    "    ) -> None:\n",
    "        self.model_id = model\n",
    "        self.device = device\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.model = BGEM3FlagModel(\n",
    "            model,\n",
    "            device=device,\n",
    "            use_fp16=use_fp16,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "        )\n",
    "\n",
    "    def encode(\n",
    "        self, text_or_texts: Union[str, List[str]], batch_size: Optional[int] = None\n",
    "    ) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        Encode text(s) to dense and sparse vectors.\n",
    "\n",
    "        Args:\n",
    "            text_or_texts: Single text string or list of texts\n",
    "            batch_size: Batch size for encoding\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with 'dense_vecs' and 'lexical_weights' keys\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(text_or_texts, str):\n",
    "                texts = [text_or_texts]\n",
    "            else:\n",
    "                texts = text_or_texts\n",
    "\n",
    "            if not texts:\n",
    "                raise ValueError(\"No texts provided for encoding\")\n",
    "\n",
    "            out = self.model.encode(\n",
    "                sentences=texts,\n",
    "                batch_size=batch_size or self.batch_size,\n",
    "                max_length=self.max_length,\n",
    "                return_dense=True,\n",
    "                return_sparse=True,\n",
    "                return_colbert_vecs=False,\n",
    "            )\n",
    "\n",
    "            # Ensure consistent data types (convert to float32 if needed)\n",
    "            if \"dense_vecs\" in out:\n",
    "                import numpy as np\n",
    "\n",
    "                out[\"dense_vecs\"] = out[\"dense_vecs\"].astype(np.float32)\n",
    "\n",
    "            # Validate output structure\n",
    "            if \"dense_vecs\" not in out or \"lexical_weights\" not in out:\n",
    "                raise ValueError(\"Invalid output from BGE-M3 model\")\n",
    "\n",
    "            # out keys: \"dense_vecs\": np.ndarray [n, d], \"lexical_weights\": List[Dict[token_id->weight]]\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error encoding with BGE-M3: {e}\")\n",
    "            # Return empty structure on error\n",
    "            import numpy as np\n",
    "\n",
    "            return {\n",
    "                \"dense_vecs\": np.array([]).astype(np.float32),\n",
    "                \"lexical_weights\": [],\n",
    "            }\n",
    "\n",
    "    def encode_query(self, text: str) -> Dict[str, List]:\n",
    "        \"\"\"Encode a single query text.\"\"\"\n",
    "        return self.encode(text)\n",
    "\n",
    "\n",
    "def load_encoder() -> BGEM3Encoder:\n",
    "    \"\"\"Load BGE-M3 encoder\"\"\"\n",
    "    return BGEM3Encoder(\n",
    "        model=EMBED_MODEL_ID,\n",
    "        device=\"cpu\",\n",
    "        normalize_embeddings=ENCODE_KWARGS.get(\"normalize_embeddings\", True),\n",
    "        batch_size=ENCODE_KWARGS.get(\"batch_size\", 32),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_hybrid_vectorstore(documents: List[Document], batch_size: int = 32) -> bool:\n",
    "    \"\"\"Build hybrid vectorstore with automatic Docker/local detection\"\"\"\n",
    "    if not documents:\n",
    "        print(\"âŒ No documents to process\")\n",
    "        return False\n",
    "\n",
    "    print(f\"ðŸ”„ Building hybrid vectorstore with {len(documents)} documents...\")\n",
    "\n",
    "    # Get client with automatic connection management\n",
    "    client, supports_hnsw = get_milvus_client()\n",
    "\n",
    "    # Create collection\n",
    "    ensure_hybrid_collection(client, COLLECTION_NAME, EMBEDDING_DIM)\n",
    "\n",
    "    # Load encoder\n",
    "    encoder = load_encoder()\n",
    "\n",
    "    # Process documents in batches\n",
    "    total = len(documents)\n",
    "    total_batches = (total + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx, start in enumerate(range(0, total, batch_size)):\n",
    "        end = min(start + batch_size, total)\n",
    "        batch_docs = documents[start:end]\n",
    "        texts = [d.page_content for d in batch_docs]\n",
    "\n",
    "        print(\n",
    "            f\"ðŸ“¦ Processing batch {batch_idx + 1}/{total_batches} ({len(batch_docs)} docs)\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Encode texts\n",
    "            emb = encoder.encode(texts, batch_size=len(texts))\n",
    "            dense = emb[\"dense_vecs\"]\n",
    "            sparse = emb[\"lexical_weights\"]\n",
    "\n",
    "            # Insert into Milvus\n",
    "            insert_documents(client, COLLECTION_NAME, dense, sparse, batch_docs)\n",
    "\n",
    "            # Clean up memory\n",
    "            del emb, dense, sparse, batch_docs, texts\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing batch {batch_idx + 1}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Build indexes\n",
    "    print(\"ðŸ”§ Building indexes...\")\n",
    "    build_indexes(client, COLLECTION_NAME)\n",
    "\n",
    "    index_type = \"HNSW\" if supports_hnsw else \"IVF_FLAT\"\n",
    "    print(f\"âœ… Hybrid vectorstore built successfully with {index_type} indexing!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acee69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = build_hybrid_vectorstore(all_documents, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b2ed2",
   "metadata": {},
   "source": [
    "___\n",
    "## 3. Retrieve from RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ee49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import Client, types\n",
    "from FlagEmbedding import FlagReranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd565754",
   "metadata": {},
   "source": [
    "### 3.1 LLM Definition and Call Managements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74526ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLM(ABC):\n",
    "    \"\"\"Base class for all LLM implementations\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        api_key: Optional[str] = None,\n",
    "        max_tokens: int = MAX_OUTPUT_TOKENS,\n",
    "        temperature: float = TEMPERATURE,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize LLM with common parameters.\n",
    "\n",
    "        Args:\n",
    "            model_id: Model identifier\n",
    "            api_key: API key for the service\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.api_key = api_key\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate response from prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "\n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class GeminiLLM:\n",
    "    \"\"\"Google Gemini LLM implementation\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"gemini-2.5-flash\",\n",
    "        api_key: Optional[str] = None,\n",
    "        max_tokens: int = MAX_OUTPUT_TOKENS,\n",
    "        temperature: float = TEMPERATURE,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Gemini LLM.\n",
    "\n",
    "        Args:\n",
    "            model_id: Gemini model identifier\n",
    "            api_key: API key (gets from env if None)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "        \"\"\"\n",
    "        load_dotenv()\n",
    "        self.model_id = model_id\n",
    "        self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY is required\")\n",
    "\n",
    "        self.llm = Client(api_key=self.api_key)\n",
    "        self.config = types.GenerateContentConfig(\n",
    "            system_instruction=\"You are a helpful assistant.\",\n",
    "            max_output_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            # thinking_config=types.ThinkingConfig(\n",
    "            #     max_steps=5,\n",
    "            #     stop_sequences=[\"\\n\"],\n",
    "            # ),\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate response from prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "\n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.llm.models.generate_content(\n",
    "                model=self.model_id,\n",
    "                contents=prompt,\n",
    "                config=self.config,\n",
    "            )\n",
    "            result = response.text.strip()\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "\n",
    "\n",
    "class LLMFactory:\n",
    "    \"\"\"Factory for creating LLM model instances\"\"\"\n",
    "\n",
    "    # Register available models here\n",
    "    MODELS = {\n",
    "        \"gemini\": GeminiLLM,\n",
    "        # \"watsonx\": WatsonxLLM,\n",
    "        # \"your_model\": YourModelLLM,\n",
    "        # Add more models here\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def create_llm(\n",
    "        self, model_type: str = \"gemini\", model_id: Optional[str] = None, **kwargs\n",
    "    ) -> BaseLLM:\n",
    "        \"\"\"\n",
    "        Create an LLM instance.\n",
    "\n",
    "        Args:\n",
    "            model_type: Type of model (\"gemini\", \"your_model\", etc.)\n",
    "            model_id: Specific model ID (uses default if None)\n",
    "            **kwargs: Additional parameters for the model\n",
    "\n",
    "        Returns:\n",
    "            Initialized LLM instance\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If model_type is not supported\n",
    "        \"\"\"\n",
    "        if model_type not in self.MODELS:\n",
    "            available = \", \".join(self.MODELS.keys())\n",
    "            raise ValueError(\n",
    "                f\"Model '{model_type}' not supported. Available: {available}\"\n",
    "            )\n",
    "\n",
    "        model_class = self.MODELS[model_type]\n",
    "\n",
    "        # Set default model_id based on type\n",
    "        if model_id is None:\n",
    "            defaults = {\n",
    "                \"gemini\": \"gemini-2.5-flash\",\n",
    "                # \"watsonx\": \"ibm/granite-13b-chat-v2\",\n",
    "                # Add defaults for other models\n",
    "            }\n",
    "            model_id = defaults.get(model_type, \"default-model\")\n",
    "\n",
    "        return model_class(model_id=model_id, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def list_available_models(cls) -> list:\n",
    "        \"\"\"Get list of available model types\"\"\"\n",
    "        return list(cls.MODELS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGLLMCaller:\n",
    "    \"\"\"\n",
    "    All-in-one LLM caller: prepare prompt, init model, call LLM, return structured output.\n",
    "\n",
    "    Now supports multiple model types through the factory system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_type: str = \"gemini\", **model_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize RAG LLM caller.\n",
    "\n",
    "        Args:\n",
    "            model_type: Type of model to use (\"gemini\", etc.)\n",
    "            **model_kwargs: Additional parameters for the model\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model_kwargs = model_kwargs\n",
    "        self.llm: Optional[BaseLLM] = None\n",
    "\n",
    "    def _init_llm(self, **kwargs):\n",
    "        \"\"\"Initialize LLM if not already done\"\"\"\n",
    "        if self.llm is None:\n",
    "            # Combine init kwargs with runtime kwargs\n",
    "            combined_kwargs = {**self.model_kwargs, **kwargs}\n",
    "            self.llm = LLMFactory.create_llm(self.model_type, **combined_kwargs)\n",
    "\n",
    "    def _prepare_context(self, documents: List[Document]) -> str:\n",
    "        \"\"\"Format documents into context string\"\"\"\n",
    "        if not documents:\n",
    "            return \"KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan.\"\n",
    "\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            content = doc.page_content.strip()\n",
    "            if content:\n",
    "                context_parts.append(f\"[TÃ i liá»‡u {i}]:\\n{content}\")\n",
    "\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    def _prepare_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create final prompt from query and context\"\"\"\n",
    "        return PROMPT.format(context=context, query=query)\n",
    "\n",
    "    def generate_answer(\n",
    "        self, query: str, documents: List[Document], **llm_kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete pipeline: documents â†’ context â†’ prompt â†’ LLM â†’ structured output\n",
    "\n",
    "        Args:\n",
    "            query: User question\n",
    "            documents: Retrieved documents\n",
    "            **llm_kwargs: Optional LLM parameters (model_id, max_tokens, temperature)\n",
    "\n",
    "        Returns:\n",
    "            Dict with answer, context_length, success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Prepare context\n",
    "            context = self._prepare_context(documents)\n",
    "\n",
    "            # Step 2: Prepare prompt\n",
    "            prompt = self._prepare_prompt(query, context)\n",
    "\n",
    "            # Step 3: Initialize LLM\n",
    "            self._init_llm(**llm_kwargs)\n",
    "\n",
    "            # Step 4: Call model\n",
    "            answer = self.llm.generate(prompt)\n",
    "\n",
    "            # Step 5: Return structured output\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"context_length\": len(context),\n",
    "                \"model_type\": self.model_type,\n",
    "                \"success\": True,\n",
    "                \"error\": None,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Xin lá»—i, cÃ³ lá»—i xáº£y ra: {str(e)}\",\n",
    "                \"context_length\": 0,\n",
    "                \"model_type\": self.model_type,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "\n",
    "# Global instance with default model\n",
    "_rag_llm_caller = None\n",
    "\n",
    "\n",
    "def get_rag_llm_caller(model_type: str = \"gemini\", **kwargs) -> RAGLLMCaller:\n",
    "    \"\"\"\n",
    "    Get global RAG LLM caller instance.\n",
    "\n",
    "    Args:\n",
    "        model_type: Type of model to use\n",
    "        **kwargs: Model parameters\n",
    "\n",
    "    Returns:\n",
    "        RAGLLMCaller instance\n",
    "    \"\"\"\n",
    "    global _rag_llm_caller\n",
    "    if _rag_llm_caller is None or _rag_llm_caller.model_type != model_type:\n",
    "        _rag_llm_caller = RAGLLMCaller(model_type=model_type, **kwargs)\n",
    "    return _rag_llm_caller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dcac6",
   "metadata": {},
   "source": [
    "### 3.2 Load retriever and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retrieval_models() -> Tuple[Any, Any, Any]:\n",
    "    \"\"\"\n",
    "    Load and initialize all models needed for RAG retrieval with PyMilvus.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing: Milvus client, embedding model, and reranker model\n",
    "    \"\"\"\n",
    "    # Clear memory before loading models\n",
    "    gc.collect()\n",
    "\n",
    "    # Load BGE-M3 embedding model\n",
    "    embedding_model = BGEM3Encoder(\n",
    "        model=EMBED_MODEL_ID,\n",
    "        device=\"cpu\",  # Force CPU to avoid memory conflicts\n",
    "        normalize_embeddings=True,\n",
    "        use_fp16=False,  # Set to False to avoid dtype issues\n",
    "        max_length=512,\n",
    "        batch_size=16,  # Smaller batch size for memory efficiency\n",
    "    )\n",
    "\n",
    "    # Test embedding model with minimal example\n",
    "    test_result = embedding_model.encode(\"test\")\n",
    "\n",
    "    # Clean up test embedding immediately\n",
    "    del test_result\n",
    "    gc.collect()\n",
    "\n",
    "    # Initialize Milvus client\n",
    "    client = MilvusClient(uri=str(MILVUS_URI))\n",
    "\n",
    "    # Test connection\n",
    "    if client.has_collection(COLLECTION_NAME):\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    # Load BGE reranker model\n",
    "    reranker_model = FlagReranker(\n",
    "        RERANKER_MODEL_ID,\n",
    "        normalize=True,\n",
    "        use_fp16=False,  # Disable FP16 to avoid memory issues\n",
    "        device=\"cpu\",  # Force CPU to avoid CUDA OOM\n",
    "    )\n",
    "\n",
    "    # Final memory cleanup\n",
    "    gc.collect()\n",
    "\n",
    "    return client, embedding_model, reranker_model\n",
    "\n",
    "\n",
    "def get_embedding_model():\n",
    "    \"\"\"Load and return BGE-M3 embedding model.\"\"\"\n",
    "\n",
    "    # Clear memory before loading models\n",
    "    gc.collect()\n",
    "\n",
    "    embedding_model = BGEM3Encoder(\n",
    "        model=EMBED_MODEL_ID,\n",
    "        device=\"cpu\",  # Force CPU to avoid memory conflicts\n",
    "        normalize_embeddings=True,\n",
    "        use_fp16=False,  # Set to False to avoid dtype issues\n",
    "        max_length=512,\n",
    "        batch_size=16,  # Smaller batch size for memory efficiency\n",
    "    )\n",
    "\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "def get_reranker_model():\n",
    "    \"\"\"Load and return BGE reranker model.\"\"\"\n",
    "\n",
    "    reranker_model = FlagReranker(\n",
    "        RERANKER_MODEL_ID,\n",
    "        normalize=True,\n",
    "        use_fp16=False,  # Disable FP16 to avoid memory issues\n",
    "        device=\"cpu\",  # Force CPU to avoid CUDA OOM\n",
    "    )\n",
    "\n",
    "    return reranker_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe84a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    \"\"\"Internal subclass for document retrieval operations.\"\"\"\n",
    "\n",
    "    def __init__(self, parent_rag):\n",
    "        \"\"\"Initialize retriever with reference to parent RAG system.\"\"\"\n",
    "        self.parent = parent_rag\n",
    "\n",
    "    def retrieve_and_rerank(self, query: str) -> Tuple[List[Document], List[float]]:\n",
    "        \"\"\"\n",
    "        Retrieve and rerank documents for a query.\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (documents, rerank_scores)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embeddings first\n",
    "            embeddings = self.parent.embedding_model.encode([query])\n",
    "            query_embedding = embeddings[\"dense_vecs\"][0]\n",
    "            query_sparse = embeddings[\"lexical_weights\"][0]\n",
    "\n",
    "            # Perform hybrid search with embeddings\n",
    "            search_results = hybrid_search(\n",
    "                client=self.parent.client,\n",
    "                collection_name=self.parent.collection_name,\n",
    "                query_embedding=query_embedding,\n",
    "                query_sparse=query_sparse,\n",
    "                k=self.parent.k,\n",
    "                similarity_threshold=self.parent.similarity_threshold,\n",
    "            )\n",
    "\n",
    "            # Convert search results to documents and perform reranking\n",
    "            documents = []\n",
    "            scores = []\n",
    "            for result in search_results:\n",
    "                # Extract text and metadata from entity field\n",
    "                entity = result.get(\"entity\", {})\n",
    "                text_content = entity.get(\"text\", \"\")\n",
    "                metadata = entity.get(\"metadata\", {})\n",
    "\n",
    "                doc = Document(\n",
    "                    page_content=text_content,\n",
    "                    metadata=metadata,\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                # Use the combined score from RRF or the available score\n",
    "                score = result.get(\n",
    "                    \"rrf_score\",\n",
    "                    result.get(\"combined_score\", result.get(\"dense_score\", 0.0)),\n",
    "                )\n",
    "                scores.append(score)\n",
    "\n",
    "            # Perform reranking if we have documents and a reranker model\n",
    "            if documents and self.parent.reranker_model:\n",
    "                try:\n",
    "                    # Prepare texts for reranking\n",
    "                    texts = [doc.page_content for doc in documents]\n",
    "\n",
    "                    # Rerank documents\n",
    "                    rerank_results = self.parent.reranker_model.compute_score(\n",
    "                        [[query, text] for text in texts]\n",
    "                    )\n",
    "\n",
    "                    # Get rerank scores and sort\n",
    "                    if isinstance(rerank_results, list):\n",
    "                        rerank_scores = rerank_results\n",
    "                    else:\n",
    "                        rerank_scores = rerank_results.tolist()\n",
    "\n",
    "                    # Sort by rerank scores and take top k\n",
    "                    scored_docs = list(zip(documents, rerank_scores))\n",
    "                    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                    # Take top rerank_top_k results\n",
    "                    top_k = min(len(scored_docs), self.parent.rerank_top_k)\n",
    "                    top_docs = scored_docs[:top_k]\n",
    "\n",
    "                    documents = [doc for doc, _ in top_docs]\n",
    "                    rerank_scores = [score for _, score in top_docs]\n",
    "\n",
    "                except Exception as e:\n",
    "                    rerank_scores = scores[: self.parent.rerank_top_k]\n",
    "                    documents = documents[: self.parent.rerank_top_k]\n",
    "            else:\n",
    "                # No reranking, just take top results\n",
    "                top_k = min(len(documents), self.parent.rerank_top_k)\n",
    "                documents = documents[:top_k]\n",
    "                rerank_scores = scores[:top_k]\n",
    "\n",
    "            return documents, rerank_scores\n",
    "\n",
    "        except Exception as e:\n",
    "            return [], []\n",
    "\n",
    "\n",
    "class AnswerGenerator:\n",
    "    \"\"\"Internal subclass for answer generation operations.\"\"\"\n",
    "\n",
    "    def __init__(self, parent_rag):\n",
    "        \"\"\"Initialize generator with reference to parent RAG system.\"\"\"\n",
    "        self.parent = parent_rag\n",
    "\n",
    "    def generate_answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "        rerank_scores: Optional[List[float]] = None,\n",
    "        **llm_kwargs,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer from query and retrieved documents.\n",
    "\n",
    "        Args:\n",
    "            query: User question\n",
    "            documents: Retrieved documents\n",
    "            rerank_scores: Reranking scores\n",
    "            **llm_kwargs: Additional LLM parameters\n",
    "\n",
    "        Returns:\n",
    "            Dict with answer, sources, confidence, etc.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"answer\": \"TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a báº¡n.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"success\": False,\n",
    "                \"retrieval_count\": 0,\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Generate answer using LLM\n",
    "            result = self.parent.llm_caller.generate_answer(\n",
    "                query=query,\n",
    "                documents=documents,\n",
    "                rerank_scores=rerank_scores,\n",
    "                **llm_kwargs,\n",
    "            )\n",
    "\n",
    "            # Add retrieval metadata\n",
    "            result[\"retrieval_count\"] = len(documents)\n",
    "            result[\"success\"] = True\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": \"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"retrieval_count\": len(documents),\n",
    "            }\n",
    "\n",
    "    def switch_model(self, model_type: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Switch LLM model.\n",
    "\n",
    "        Args:\n",
    "            model_type: New model type\n",
    "            **model_kwargs: Model parameters\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create new LLM caller with specified model\n",
    "            self.parent.llm_caller = get_rag_llm_caller(\n",
    "                model_type=model_type, **model_kwargs\n",
    "            )\n",
    "\n",
    "            # Update parent's model type tracking\n",
    "            self.parent._current_model_type = model_type\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "\n",
    "class VietnameseRAG:\n",
    "    \"\"\"\n",
    "    Unified Vietnamese RAG system with modular subclasses.\n",
    "\n",
    "    This class provides the complete RAG flow while using internal\n",
    "    subclasses for clean modular architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Connection parameters\n",
    "        client=None,\n",
    "        collection_name: str = COLLECTION_NAME,\n",
    "        # Model parameters\n",
    "        embedding_model=None,\n",
    "        reranker_model=None,\n",
    "        # Retrieval parameters\n",
    "        k: int = DEFAULT_K,\n",
    "        rerank_top_k: int = RERANK_TOP_K,\n",
    "        similarity_threshold: float = SIMILARITY_THRESHOLD,\n",
    "        # LLM parameters\n",
    "        model_type: str = \"gemini\",\n",
    "        llm_caller: Optional[RAGLLMCaller] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize unified Vietnamese RAG system.\n",
    "\n",
    "        Args:\n",
    "            client: Milvus client\n",
    "            collection_name: Collection name in Milvus\n",
    "            embedding_model: BGE-M3 embedding model\n",
    "            reranker_model: Reranker model\n",
    "            k: Initial retrieval count\n",
    "            rerank_top_k: Final count after reranking\n",
    "            similarity_threshold: Minimum similarity threshold\n",
    "            model_type: LLM model type (gemini, watsonx, etc.)\n",
    "            llm_caller: Custom LLM caller instance\n",
    "            **llm_kwargs: Additional LLM parameters\n",
    "        \"\"\"\n",
    "        # Store configuration\n",
    "        self.collection_name = collection_name\n",
    "        self.k = k\n",
    "        self.rerank_top_k = rerank_top_k\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self._current_model_type = model_type\n",
    "\n",
    "        # Initialize Milvus client\n",
    "        self.client = client or get_milvus_client()\n",
    "\n",
    "        # Load models\n",
    "        self.embedding_model = embedding_model or get_embedding_model()\n",
    "        self.reranker_model = reranker_model or get_reranker_model()\n",
    "\n",
    "        # Initialize LLM caller\n",
    "        self.llm_caller = llm_caller or get_rag_llm_caller(\n",
    "            model_type=model_type, **llm_kwargs\n",
    "        )\n",
    "\n",
    "        # Initialize modular subclasses\n",
    "        self.retriever = DocumentRetriever(self)\n",
    "        self.generator = AnswerGenerator(self)\n",
    "\n",
    "    def answer(self, query: str, **llm_kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Main method: Complete RAG flow from query to answer.\n",
    "\n",
    "        Args:\n",
    "            query: User question\n",
    "            **llm_kwargs: Additional LLM parameters\n",
    "\n",
    "        Returns:\n",
    "            Complete result with answer, sources, confidence, etc.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Retrieve and rerank documents\n",
    "            documents, rerank_scores = self.retriever.retrieve_and_rerank(query)\n",
    "\n",
    "            if not documents:\n",
    "                return {\n",
    "                    \"answer\": \"TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a báº¡n.\",\n",
    "                    \"sources\": [],\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"success\": False,\n",
    "                    \"retrieval_count\": 0,\n",
    "                }\n",
    "\n",
    "            # Step 2: Generate answer\n",
    "            result = self.generator.generate_answer(\n",
    "                query=query,\n",
    "                documents=documents,\n",
    "                rerank_scores=rerank_scores,\n",
    "                **llm_kwargs,\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": \"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"retrieval_count\": 0,\n",
    "            }\n",
    "\n",
    "    def switch_model(self, model_type: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Switch LLM model.\n",
    "\n",
    "        Args:\n",
    "            model_type: New model type (gemini, watsonx, etc.)\n",
    "            **model_kwargs: Model-specific parameters\n",
    "        \"\"\"\n",
    "        self.generator.switch_model(model_type, **model_kwargs)\n",
    "\n",
    "    @property\n",
    "    def model_type(self) -> str:\n",
    "        \"\"\"Get current LLM model type.\"\"\"\n",
    "        return self._current_model_type\n",
    "\n",
    "    @property\n",
    "    def status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system status and configuration.\"\"\"\n",
    "        return {\n",
    "            \"model_type\": self.model_type,\n",
    "            \"collection_name\": self.collection_name,\n",
    "            \"retrieval_config\": {\n",
    "                \"k\": self.k,\n",
    "                \"rerank_top_k\": self.rerank_top_k,\n",
    "                \"similarity_threshold\": self.similarity_threshold,\n",
    "            },\n",
    "            \"models_loaded\": {\n",
    "                \"embedding\": self.embedding_model is not None,\n",
    "                \"reranker\": self.reranker_model is not None,\n",
    "                \"llm\": self.llm_caller is not None,\n",
    "            },\n",
    "            \"milvus_connected\": self.client is not None,\n",
    "        }\n",
    "\n",
    "    def update_config(\n",
    "        self,\n",
    "        k: Optional[int] = None,\n",
    "        rerank_top_k: Optional[int] = None,\n",
    "        similarity_threshold: Optional[float] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update retrieval configuration.\n",
    "\n",
    "        Args:\n",
    "            k: New initial retrieval count\n",
    "            rerank_top_k: New reranking count\n",
    "            similarity_threshold: New similarity threshold\n",
    "        \"\"\"\n",
    "        if k is not None:\n",
    "            self.k = k\n",
    "\n",
    "        if rerank_top_k is not None:\n",
    "            self.rerank_top_k = rerank_top_k\n",
    "\n",
    "        if similarity_threshold is not None:\n",
    "            self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "# Convenience function for backward compatibility\n",
    "def get_vietnamese_rag(**kwargs) -> VietnameseRAG:\n",
    "    \"\"\"\n",
    "    Get Vietnamese RAG instance with default configuration.\n",
    "\n",
    "    Args:\n",
    "        **kwargs: Configuration parameters\n",
    "\n",
    "    Returns:\n",
    "        VietnameseRAG instance\n",
    "    \"\"\"\n",
    "    return VietnameseRAG(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1742f49",
   "metadata": {},
   "source": [
    "### 3.3 Define RAG Chain and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_retrieval(rag, query: str) -> Tuple[List[Document], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform document retrieval and reranking only.\n",
    "\n",
    "    Args:\n",
    "        rag: The Vietnamese RAG system instance\n",
    "        query: The search query\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (documents, rerank_scores)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the retriever directly for cleaner separation\n",
    "        documents, scores = rag.retriever.retrieve_and_rerank(query)\n",
    "\n",
    "        return documents, scores\n",
    "\n",
    "    except Exception as e:\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def generate_answer(rag, query: str, documents: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using LLM based on retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        rag: The Vietnamese RAG system instance\n",
    "        query: The search query\n",
    "        documents: Retrieved documents\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing answer and metadata\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"No documents provided for answer generation\",\n",
    "            \"answer\": None,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Use the answer generator directly\n",
    "        answer_result = rag.answer_generator.generate_answer(query, documents)\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"answer\": answer_result.get(\"answer\", \"\"),\n",
    "            \"confidence\": answer_result.get(\"confidence\", 0.0),\n",
    "            \"error\": None,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"answer\": None}\n",
    "\n",
    "\n",
    "def display_results(\n",
    "    query: str,\n",
    "    documents: List[Document],\n",
    "    scores: List[float],\n",
    "    answer_result: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display the retrieval and answer generation results.\n",
    "\n",
    "    Args:\n",
    "        query: The search query\n",
    "        documents: Retrieved documents\n",
    "        scores: Document scores\n",
    "        answer_result: LLM answer generation result\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        print(\"âŒ No relevant documents found.\")\n",
    "        return\n",
    "\n",
    "    # If answer generation succeeded, show only query and answer\n",
    "    if answer_result.get(\"success\") and answer_result.get(\"answer\"):\n",
    "        print(f\"\\nðŸ” **Query:** {query}\")\n",
    "        print(f\"\\nðŸ¤– **Answer:**\")\n",
    "        print(\"=\" * 50)\n",
    "        print(answer_result[\"answer\"])\n",
    "\n",
    "        # Log the successful Q&A pair\n",
    "        retrieval_info = {\n",
    "            \"num_results\": len(documents),\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"filename\": doc.metadata.get(\"source_filename\", \"Unknown\"),\n",
    "                    \"score\": score,\n",
    "                    \"content_length\": len(doc.page_content),\n",
    "                }\n",
    "                for doc, score in zip(documents, scores)\n",
    "            ],\n",
    "            \"confidence\": answer_result.get(\"confidence\", 0.0),\n",
    "        }\n",
    "\n",
    "    # If answer generation failed, show LLM calling prompt\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ **LLM calling failed or returned no answer**\")\n",
    "        if answer_result.get(\"error\"):\n",
    "            print(f\"Error: {answer_result['error']}\")\n",
    "\n",
    "        print(f\"\\nðŸ” Query: {query}\")\n",
    "        print(\n",
    "            f\"ðŸ“Š Retrieved {len(documents)} documents but LLM answer generation failed.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00694011",
   "metadata": {},
   "outputs": [],
   "source": [
    "client, supports_hnsw = get_milvus_client()\n",
    "embedding_model, reranker_model = load_retrieval_models()\n",
    "rag = VietnameseRAG(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_model=embedding_model,\n",
    "    reranker_model=reranker_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ’¡ Enter 'quit' to exit the system\")\n",
    "print(\"ðŸ¤– Complete RAG: BGE-M3 search â†’ LLM answer generation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get user input\n",
    "        query = input(\"\\nðŸ” Enter your query: \").strip()\n",
    "\n",
    "        # Check for quit command\n",
    "        if query.lower() in [\"quit\", \"q\", \"exit\"]:\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if not query:\n",
    "            print(\"âš ï¸ Please enter a valid query\")\n",
    "            continue\n",
    "\n",
    "        # Step 1: Perform retrieval\n",
    "        documents, scores = perform_retrieval(rag, query)\n",
    "\n",
    "        # Step 2: Generate answer (if documents found)\n",
    "        answer_result = generate_answer(rag, query, documents)\n",
    "\n",
    "        # Step 3: Display results\n",
    "        display_results(query, documents, scores, answer_result)\n",
    "\n",
    "        # Clean up after each query\n",
    "        gc.collect()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nðŸ‘‹ Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing query: {e}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
