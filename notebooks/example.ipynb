{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc3a803",
   "metadata": {},
   "source": [
    "# LangChain Docling RAG Example\n",
    "\n",
    "This notebook demonstrates how to use LangChain's Docling integration for RAG (Retrieval Augmented Generation) pipeline.\n",
    "Based on the official LangChain Docling documentation: https://python.langchain.com/docs/integrations/document_loaders/docling/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f3891",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load configuration from config.json file including API keys and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "# Load configuration\n",
    "config_path = \"../config.json\"\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Environment setup for tokenizers (to avoid warnings)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Helper function to get environment variables\n",
    "def _get_env_from_colab_or_os(key):\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        try:\n",
    "            return userdata.get(key)\n",
    "        except userdata.SecretNotFoundError:\n",
    "            pass\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return os.getenv(key)\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Configuration loaded from: {config_path}\")\n",
    "print(f\"HuggingFace Token available: {'Yes' if HF_TOKEN else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f8d34",
   "metadata": {},
   "source": [
    "## 2. Install Required Dependencies\n",
    "\n",
    "Install necessary packages including langchain-docling and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qU langchain-docling\n",
    "!pip install -q --progress-bar off --no-warn-conflicts langchain-core langchain-huggingface langchain-milvus langchain python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329737ea",
   "metadata": {},
   "source": [
    "## 3. Import Libraries and Load Configuration\n",
    "\n",
    "Import all required libraries including LangChain, Docling, and load configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3789ca9",
   "metadata": {},
   "source": [
    "## 4. Configure Pipeline Parameters\n",
    "\n",
    "Set up parameters for the RAG pipeline based on configuration and LangChain Docling example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c940e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline parameters from LangChain Docling example\n",
    "FILE_PATH = [\"https://arxiv.org/pdf/2408.09869\"]  # Docling Technical Report\n",
    "EMBED_MODEL_ID = (\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\"  # You can change to BAAI model later\n",
    ")\n",
    "GEN_MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "EXPORT_TYPE = ExportType.DOC_CHUNKS  # or ExportType.MARKDOWN\n",
    "QUESTION = \"Which are the main AI models in Docling?\"\n",
    "TOP_K = 3\n",
    "MILVUS_URI = str(Path(mkdtemp()) / \"docling.db\")  # Temporary database\n",
    "\n",
    "# Prompt template\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, answer the query.\\n\"\n",
    "    \"Query: {input}\\n\"\n",
    "    \"Answer:\\n\"\n",
    ")\n",
    "\n",
    "print(f\"File path: {FILE_PATH}\")\n",
    "print(f\"Embedding model: {EMBED_MODEL_ID}\")\n",
    "print(f\"Export type: {EXPORT_TYPE}\")\n",
    "print(f\"Milvus URI: {MILVUS_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31e7d8",
   "metadata": {},
   "source": [
    "## 5. Initialize Docling Document Loader\n",
    "\n",
    "Set up the DoclingLoader with proper configuration for document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891801e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DoclingLoader with HybridChunker\n",
    "loader = DoclingLoader(\n",
    "    file_path=FILE_PATH,\n",
    "    export_type=EXPORT_TYPE,\n",
    "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
    ")\n",
    "\n",
    "print(\"DoclingLoader initialized successfully!\")\n",
    "print(f\"Export type: {EXPORT_TYPE}\")\n",
    "print(f\"Chunker: HybridChunker with tokenizer {EMBED_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268339f4",
   "metadata": {},
   "source": [
    "## 6. Load and Process Documents\n",
    "\n",
    "Use Docling loader to load and process documents from specified paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b96bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "print(\"Loading documents...\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Inspect first few documents\n",
    "for i, d in enumerate(docs[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content preview: {d.page_content[:100]}...\")\n",
    "    print(f\"Metadata keys: {list(d.metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f26d2b",
   "metadata": {},
   "source": [
    "## 7. Determine Document Splits\n",
    "\n",
    "Process documents into splits based on the export type (DOC_CHUNKS or MARKDOWN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine splits based on export type\n",
    "if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n",
    "    splits = docs\n",
    "    print(f\"Using DOC_CHUNKS: {len(splits)} splits\")\n",
    "elif EXPORT_TYPE == ExportType.MARKDOWN:\n",
    "    # Use MarkdownHeaderTextSplitter for Markdown export\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header_1\"),\n",
    "            (\"##\", \"Header_2\"),\n",
    "            (\"###\", \"Header_3\"),\n",
    "        ],\n",
    "    )\n",
    "    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
    "    print(f\"Using MARKDOWN with header splitting: {len(splits)} splits\")\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")\n",
    "\n",
    "# Inspect some sample splits\n",
    "print(\"\\nSample splits:\")\n",
    "for i, d in enumerate(splits[:3]):\n",
    "    print(f\"\\nSplit {i+1}: {d.page_content[:150]}...\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb062b",
   "metadata": {},
   "source": [
    "## 8. Setup Vector Store with Embeddings\n",
    "\n",
    "Initialize embedding model and Milvus vector store for document indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaaaab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "print(\"Initializing embeddings model...\")\n",
    "embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
    "\n",
    "# Create Milvus vector store from documents\n",
    "print(\"Creating Milvus vector store...\")\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"docling_demo\",\n",
    "    connection_args={\"uri\": MILVUS_URI},\n",
    "    index_params={\"index_type\": \"FLAT\"},\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "print(f\"Vector store created successfully!\")\n",
    "print(f\"Collection: docling_demo\")\n",
    "print(f\"Index type: FLAT\")\n",
    "print(f\"Number of documents indexed: {len(splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1ea76",
   "metadata": {},
   "source": [
    "## 9. Setup Retrieval Chain\n",
    "\n",
    "Create a retrieval system using the processed documents and vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa72bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "# Initialize LLM (using HuggingFace Endpoint)\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=GEN_MODEL_ID,\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    task=\"text-generation\",\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "print(\"RAG chain created successfully!\")\n",
    "print(f\"Retriever top-k: {TOP_K}\")\n",
    "print(f\"LLM model: {GEN_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1efb0c",
   "metadata": {},
   "source": [
    "## 10. Test Document Loading and Retrieval\n",
    "\n",
    "Test the complete pipeline by loading sample documents and performing retrieval queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26cb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clip text for display\n",
    "def clip_text(text, threshold=100):\n",
    "    return f\"{text[:threshold]}...\" if len(text) > threshold else text\n",
    "\n",
    "\n",
    "# Run the RAG chain\n",
    "print(f\"Question: {QUESTION}\")\n",
    "print(\"\\nProcessing...\")\n",
    "\n",
    "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
    "\n",
    "# Display results\n",
    "clipped_answer = clip_text(resp_dict[\"answer\"], threshold=350)\n",
    "print(f\"\\nQuestion:\\n{resp_dict['input']}\")\n",
    "print(f\"\\nAnswer:\\n{clipped_answer}\")\n",
    "\n",
    "# Display retrieved context sources\n",
    "print(\"\\nRetrieved Sources:\")\n",
    "for i, doc in enumerate(resp_dict[\"context\"]):\n",
    "    print(f\"\\nSource {i + 1}:\")\n",
    "    print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n",
    "\n",
    "    # Display metadata (excluding 'pk' if present)\n",
    "    for key in doc.metadata:\n",
    "        if key != \"pk\":\n",
    "            val = doc.metadata.get(key)\n",
    "            clipped_val = clip_text(val) if isinstance(val, str) else val\n",
    "            print(f\"  {key}: {clipped_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e09fd",
   "metadata": {},
   "source": [
    "## 11. Alternative: Test with Local PDF Files\n",
    "\n",
    "Example of how to use the system with local PDF files instead of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87fc554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with local files (uncomment to use)\n",
    "# LOCAL_PDF_PATH = \"../data/pdf/\"  # Adjust path as needed\n",
    "#\n",
    "# # Create loader for local files\n",
    "# local_loader = DoclingLoader(\n",
    "#     file_path=LOCAL_PDF_PATH,\n",
    "#     export_type=ExportType.DOC_CHUNKS,\n",
    "#     chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
    "# )\n",
    "#\n",
    "# # Load and process local documents\n",
    "# local_docs = local_loader.load()\n",
    "# print(f\"Loaded {len(local_docs)} local documents\")\n",
    "\n",
    "print(\"Local file processing example is commented out.\")\n",
    "print(\"Uncomment the code above to test with local PDF files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e5982",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the LangChain Docling integration for RAG:\n",
    "\n",
    "1. **Document Loading**: Uses DoclingLoader to parse PDFs with rich metadata\n",
    "2. **Chunking**: Supports both DOC_CHUNKS and MARKDOWN export types\n",
    "3. **Vector Storage**: Integrates with Milvus for efficient similarity search\n",
    "4. **Retrieval**: Creates a complete RAG chain with HuggingFace models\n",
    "5. **Rich Metadata**: Preserves document structure, headings, and bounding boxes\n",
    "\n",
    "### Key Benefits of Docling:\n",
    "- Preserves document layout and structure\n",
    "- Extracts tables and other elements properly\n",
    "- Provides rich metadata for better grounding\n",
    "- Works with multiple document formats (PDF, DOCX, PPTX, etc.)\n",
    "\n",
    "### Next Steps:\n",
    "- Replace with BAAI embedding models as requested\n",
    "- Add BM25 hybrid retrieval\n",
    "- Implement Vietnamese language support\n",
    "- Add proper error handling and logging"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
